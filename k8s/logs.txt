
==> Audit <==
|---------|--------------------------------|----------|------------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |    User    | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|------------|---------|---------------------|---------------------|
| service | worldhello-helmhellowolrd      | minikube | fajarsujai | v1.30.1 | 23 Nov 23 15:39 WIB |                     |
| service | worldhello-helmhellowolrd      | minikube | fajarsujai | v1.30.1 | 23 Nov 23 15:41 WIB |                     |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 06 Dec 23 17:38 WIB | 06 Dec 23 17:39 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 07 Dec 23 12:02 WIB | 07 Dec 23 12:03 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 12 Dec 23 06:44 WIB | 12 Dec 23 06:44 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 14 Dec 23 09:21 WIB | 14 Dec 23 09:23 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 15 Dec 23 09:16 WIB | 15 Dec 23 09:17 WIB |
| ip      |                                | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:31 WIB | 15 Dec 23 11:31 WIB |
| options |                                | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:33 WIB | 15 Dec 23 11:33 WIB |
| service |                                | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:34 WIB |                     |
| service | ip                             | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:34 WIB |                     |
| ip      | service                        | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:34 WIB | 15 Dec 23 11:34 WIB |
| service | ip -n develop-ghw              | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:34 WIB |                     |
| ip      | -n develop-ghw                 | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:35 WIB |                     |
| service | ip -n develop-ghw              | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:36 WIB |                     |
| service | ip ghw-service                 | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:36 WIB |                     |
| service | ip ghw-service -n develop-ghs  | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:37 WIB |                     |
| service | ghw-service -n develop-ghs     | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:37 WIB |                     |
| service | list                           | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:37 WIB | 15 Dec 23 11:37 WIB |
| service | ghw-service                    | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:37 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 15 Dec 23 11:38 WIB |                     |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 19 Dec 23 09:34 WIB | 19 Dec 23 09:35 WIB |
| service | ip ghw-service                 | minikube | fajarsujai | v1.30.1 | 19 Dec 23 11:06 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 19 Dec 23 11:06 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 19 Dec 23 11:15 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 19 Dec 23 11:23 WIB |                     |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 20 Dec 23 09:35 WIB | 20 Dec 23 09:36 WIB |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 20 Dec 23 09:48 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 20 Dec 23 10:21 WIB |                     |
| service | ghw-service ip                 | minikube | fajarsujai | v1.30.1 | 25 Dec 23 10:49 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 25 Dec 23 10:49 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 25 Dec 23 22:04 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 25 Dec 23 22:12 WIB |                     |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 29 Dec 23 09:28 WIB | 29 Dec 23 09:29 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 02 Jan 24 09:08 WIB | 02 Jan 24 09:10 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 07 Jan 24 13:49 WIB | 07 Jan 24 13:49 WIB |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 08 Jan 24 13:46 WIB | 08 Jan 24 13:47 WIB |
| service | ghw-service                    | minikube | fajarsujai | v1.30.1 | 08 Jan 24 19:24 WIB |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 08 Jan 24 19:25 WIB |                     |
| service | ghw-service-sc-vol             | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:22 WIB |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:22 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service -n develop-ghw     | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:24 WIB |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:29 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:30 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:30 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:31 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:31 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:32 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol             | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:46 WIB |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:46 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:47 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:48 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:54 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 00:57 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| start   | --driver=docker                | minikube | fajarsujai | v1.30.1 | 09 Jan 24 09:48 WIB | 09 Jan 24 09:49 WIB |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 10:20 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 11:05 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 11:06 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 11:07 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
| service | ghw-service-sc-vol -n          | minikube | fajarsujai | v1.30.1 | 09 Jan 24 15:32 WIB |                     |
|         | develop-ghw-sc-vol             |          |            |         |                     |                     |
|---------|--------------------------------|----------|------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/01/09 09:48:36
Running on machine: fajarsujai
Binary: Built with gc go1.20.2 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0109 09:48:36.931208     352 out.go:296] Setting OutFile to fd 1 ...
I0109 09:48:36.931700     352 out.go:309] Setting ErrFile to fd 2...
I0109 09:48:36.931777     352 root.go:336] Updating PATH: /home/fajarsujai/.minikube/bin
W0109 09:48:36.932138     352 root.go:312] Error reading config file at /home/fajarsujai/.minikube/config/config.json: open /home/fajarsujai/.minikube/config/config.json: no such file or directory
I0109 09:48:36.933506     352 out.go:303] Setting JSON to false
I0109 09:48:36.934955     352 start.go:125] hostinfo: {"hostname":"fajarsujai","uptime":204,"bootTime":1704768313,"procs":10,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.10.16.3-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"24e5cf43-38d2-4606-bbae-ac918304389b"}
I0109 09:48:36.934990     352 start.go:135] virtualization:  guest
I0109 09:48:36.937290     352 out.go:177] 😄  minikube v1.30.1 on Ubuntu 22.04 (amd64)
I0109 09:48:36.940128     352 notify.go:220] Checking for updates...
I0109 09:48:36.940592     352 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0109 09:48:36.943232     352 driver.go:375] Setting default libvirt URI to qemu:///system
I0109 09:48:37.053408     352 docker.go:121] docker version: linux-20.10.24:Docker Desktop
I0109 09:48:37.053489     352 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0109 09:48:37.429867     352 info.go:266] docker info: {ID:OU25:RJFZ:PBIK:ZHFH:VQ3I:3UUL:2NLF:BTMA:MYPC:EVPI:WX3B:D7QW Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:144 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:172 SystemTime:2024-01-09 02:48:37.108036773 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8212963328 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.24 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:lgn74vaxgjp9h6q304j0d9rq0 NodeAddr:192.168.65.4 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.4:2377 NodeID:lgn74vaxgjp9h6q304j0d9rq0]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2456e983eb9e37e47538f59ea18f2043c9a73640 Expected:2456e983eb9e37e47538f59ea18f2043c9a73640} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.2] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.2] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.25.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.9.0]] Warnings:<nil>}}
I0109 09:48:37.429969     352 docker.go:294] overlay module found
I0109 09:48:37.431878     352 out.go:177] ✨  Using the docker driver based on existing profile
I0109 09:48:37.433325     352 start.go:295] selected driver: docker
I0109 09:48:37.433329     352 start.go:870] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/fajarsujai:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0109 09:48:37.433406     352 start.go:881] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0109 09:48:37.433468     352 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0109 09:48:37.530266     352 lock.go:35] WriteFile acquiring /home/fajarsujai/.minikube/last_update_check: {Name:mk37e8aeae29c9b48b757a59d4368c3843d00eb8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0109 09:48:37.533291     352 out.go:177] 🎉  minikube 1.32.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.32.0
I0109 09:48:37.535044     352 out.go:177] 💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0109 09:48:37.622740     352 info.go:266] docker info: {ID:OU25:RJFZ:PBIK:ZHFH:VQ3I:3UUL:2NLF:BTMA:MYPC:EVPI:WX3B:D7QW Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:144 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:172 SystemTime:2024-01-09 02:48:37.493365755 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8212963328 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.24 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:lgn74vaxgjp9h6q304j0d9rq0 NodeAddr:192.168.65.4 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.4:2377 NodeID:lgn74vaxgjp9h6q304j0d9rq0]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2456e983eb9e37e47538f59ea18f2043c9a73640 Expected:2456e983eb9e37e47538f59ea18f2043c9a73640} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.2] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.2] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.25.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.9.0]] Warnings:<nil>}}
I0109 09:48:37.623250     352 cni.go:84] Creating CNI manager for ""
I0109 09:48:37.623262     352 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0109 09:48:37.623269     352 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/fajarsujai:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0109 09:48:37.626461     352 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0109 09:48:37.628840     352 cache.go:120] Beginning downloading kic base image for docker with docker
I0109 09:48:37.630668     352 out.go:177] 🚜  Pulling base image ...
I0109 09:48:37.632079     352 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I0109 09:48:37.632115     352 preload.go:148] Found local preload: /home/fajarsujai/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-amd64.tar.lz4
I0109 09:48:37.632120     352 cache.go:57] Caching tarball of preloaded images
I0109 09:48:37.632190     352 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.39 in local docker daemon
I0109 09:48:37.632287     352 preload.go:174] Found /home/fajarsujai/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0109 09:48:37.632316     352 cache.go:60] Finished verifying existence of preloaded tar for  v1.26.3 on docker
I0109 09:48:37.632385     352 profile.go:148] Saving config to /home/fajarsujai/.minikube/profiles/minikube/config.json ...
I0109 09:48:37.720522     352 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.39 in local docker daemon, skipping pull
I0109 09:48:37.720557     352 cache.go:143] gcr.io/k8s-minikube/kicbase:v0.0.39 exists in daemon, skipping load
I0109 09:48:37.720596     352 cache.go:193] Successfully downloaded all kic artifacts
I0109 09:48:37.720634     352 start.go:364] acquiring machines lock for minikube: {Name:mk33bd60103c4602b94baaf32401a0d80746d889 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0109 09:48:37.720743     352 start.go:368] acquired machines lock for "minikube" in 94.411µs
I0109 09:48:37.720756     352 start.go:96] Skipping create...Using existing machine configuration
I0109 09:48:37.720759     352 fix.go:55] fixHost starting: 
I0109 09:48:37.720960     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:48:37.789385     352 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0109 09:48:37.789405     352 fix.go:129] unexpected machine state, will restart: <nil>
I0109 09:48:37.792772     352 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0109 09:48:37.794326     352 cli_runner.go:164] Run: docker start minikube
I0109 09:48:39.683940     352 cli_runner.go:217] Completed: docker start minikube: (1.88958343s)
I0109 09:48:39.684041     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:48:39.788772     352 kic.go:426] container "minikube" state is running.
I0109 09:48:39.789080     352 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0109 09:48:39.861582     352 profile.go:148] Saving config to /home/fajarsujai/.minikube/profiles/minikube/config.json ...
I0109 09:48:39.861764     352 machine.go:88] provisioning docker machine ...
I0109 09:48:39.861782     352 ubuntu.go:169] provisioning hostname "minikube"
I0109 09:48:39.861825     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:39.935834     352 main.go:141] libmachine: Using SSH client type: native
I0109 09:48:39.936938     352 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 58376 <nil> <nil>}
I0109 09:48:39.936948     352 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0109 09:48:39.937842     352 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0109 09:48:43.121645     352 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0109 09:48:43.121711     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:43.198119     352 main.go:141] libmachine: Using SSH client type: native
I0109 09:48:43.198525     352 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 58376 <nil> <nil>}
I0109 09:48:43.198536     352 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0109 09:48:43.337181     352 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0109 09:48:43.337204     352 ubuntu.go:175] set auth options {CertDir:/home/fajarsujai/.minikube CaCertPath:/home/fajarsujai/.minikube/certs/ca.pem CaPrivateKeyPath:/home/fajarsujai/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/fajarsujai/.minikube/machines/server.pem ServerKeyPath:/home/fajarsujai/.minikube/machines/server-key.pem ClientKeyPath:/home/fajarsujai/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/fajarsujai/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/fajarsujai/.minikube}
I0109 09:48:43.337250     352 ubuntu.go:177] setting up certificates
I0109 09:48:43.337276     352 provision.go:83] configureAuth start
I0109 09:48:43.337387     352 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0109 09:48:43.407875     352 provision.go:138] copyHostCerts
I0109 09:48:43.409228     352 exec_runner.go:144] found /home/fajarsujai/.minikube/ca.pem, removing ...
I0109 09:48:43.409236     352 exec_runner.go:207] rm: /home/fajarsujai/.minikube/ca.pem
I0109 09:48:43.409275     352 exec_runner.go:151] cp: /home/fajarsujai/.minikube/certs/ca.pem --> /home/fajarsujai/.minikube/ca.pem (1086 bytes)
I0109 09:48:43.409798     352 exec_runner.go:144] found /home/fajarsujai/.minikube/cert.pem, removing ...
I0109 09:48:43.409803     352 exec_runner.go:207] rm: /home/fajarsujai/.minikube/cert.pem
I0109 09:48:43.409826     352 exec_runner.go:151] cp: /home/fajarsujai/.minikube/certs/cert.pem --> /home/fajarsujai/.minikube/cert.pem (1131 bytes)
I0109 09:48:43.410106     352 exec_runner.go:144] found /home/fajarsujai/.minikube/key.pem, removing ...
I0109 09:48:43.410110     352 exec_runner.go:207] rm: /home/fajarsujai/.minikube/key.pem
I0109 09:48:43.410130     352 exec_runner.go:151] cp: /home/fajarsujai/.minikube/certs/key.pem --> /home/fajarsujai/.minikube/key.pem (1675 bytes)
I0109 09:48:43.410437     352 provision.go:112] generating server cert: /home/fajarsujai/.minikube/machines/server.pem ca-key=/home/fajarsujai/.minikube/certs/ca.pem private-key=/home/fajarsujai/.minikube/certs/ca-key.pem org=fajarsujai.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0109 09:48:43.461068     352 provision.go:172] copyRemoteCerts
I0109 09:48:43.461954     352 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0109 09:48:43.462000     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:43.531035     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:48:43.638868     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0109 09:48:43.684995     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I0109 09:48:43.707168     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0109 09:48:43.737098     352 provision.go:86] duration metric: configureAuth took 399.810949ms
I0109 09:48:43.737115     352 ubuntu.go:193] setting minikube options for container-runtime
I0109 09:48:43.737265     352 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0109 09:48:43.737319     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:43.827556     352 main.go:141] libmachine: Using SSH client type: native
I0109 09:48:43.828317     352 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 58376 <nil> <nil>}
I0109 09:48:43.828334     352 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0109 09:48:43.974491     352 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0109 09:48:43.974502     352 ubuntu.go:71] root file system type: overlay
I0109 09:48:43.974610     352 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0109 09:48:43.974665     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:44.048104     352 main.go:141] libmachine: Using SSH client type: native
I0109 09:48:44.048422     352 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 58376 <nil> <nil>}
I0109 09:48:44.048465     352 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0109 09:48:44.194751     352 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0109 09:48:44.194835     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:44.271294     352 main.go:141] libmachine: Using SSH client type: native
I0109 09:48:44.271627     352 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80e3e0] 0x811480 <nil>  [] 0s} 127.0.0.1 58376 <nil> <nil>}
I0109 09:48:44.271638     352 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0109 09:48:44.409093     352 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0109 09:48:44.409111     352 machine.go:91] provisioned docker machine in 4.547335771s
I0109 09:48:44.409118     352 start.go:300] post-start starting for "minikube" (driver="docker")
I0109 09:48:44.409122     352 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0109 09:48:44.409170     352 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0109 09:48:44.409207     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:44.477517     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:48:44.576915     352 ssh_runner.go:195] Run: cat /etc/os-release
I0109 09:48:44.580002     352 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0109 09:48:44.580014     352 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0109 09:48:44.580019     352 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0109 09:48:44.580023     352 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I0109 09:48:44.580030     352 filesync.go:126] Scanning /home/fajarsujai/.minikube/addons for local assets ...
I0109 09:48:44.580465     352 filesync.go:126] Scanning /home/fajarsujai/.minikube/files for local assets ...
I0109 09:48:44.580780     352 start.go:303] post-start completed in 171.656132ms
I0109 09:48:44.580835     352 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0109 09:48:44.580868     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:44.650426     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:48:44.695411     352 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0109 09:48:44.699462     352 fix.go:57] fixHost completed within 6.978695548s
I0109 09:48:44.699473     352 start.go:83] releasing machines lock for "minikube", held for 6.978724589s
I0109 09:48:44.699532     352 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0109 09:48:44.767356     352 ssh_runner.go:195] Run: cat /version.json
I0109 09:48:44.767410     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:44.767472     352 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0109 09:48:44.768248     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:48:44.848841     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:48:44.849286     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:48:45.438319     352 ssh_runner.go:195] Run: systemctl --version
I0109 09:48:45.446439     352 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0109 09:48:45.450689     352 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0109 09:48:45.469532     352 cni.go:229] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0109 09:48:45.469604     352 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0109 09:48:45.478007     352 cni.go:258] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0109 09:48:45.478022     352 start.go:481] detecting cgroup driver to use...
I0109 09:48:45.478046     352 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0109 09:48:45.478115     352 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0109 09:48:45.492597     352 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0109 09:48:45.503016     352 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0109 09:48:45.511501     352 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0109 09:48:45.511540     352 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0109 09:48:45.520112     352 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0109 09:48:45.528867     352 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0109 09:48:45.537254     352 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0109 09:48:45.547410     352 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0109 09:48:45.555186     352 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0109 09:48:45.563996     352 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0109 09:48:45.572319     352 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0109 09:48:45.579570     352 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0109 09:48:45.674067     352 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0109 09:48:45.789982     352 start.go:481] detecting cgroup driver to use...
I0109 09:48:45.790013     352 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0109 09:48:45.790062     352 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0109 09:48:45.801599     352 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0109 09:48:45.801663     352 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0109 09:48:45.813948     352 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0109 09:48:45.830236     352 ssh_runner.go:195] Run: which cri-dockerd
I0109 09:48:45.833899     352 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0109 09:48:45.843452     352 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0109 09:48:45.857440     352 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0109 09:48:46.011366     352 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0109 09:48:46.149839     352 docker.go:538] configuring docker to use "cgroupfs" as cgroup driver...
I0109 09:48:46.149852     352 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0109 09:48:46.164266     352 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0109 09:48:46.255580     352 ssh_runner.go:195] Run: sudo systemctl restart docker
I0109 09:48:50.800858     352 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.545254506s)
I0109 09:48:50.800907     352 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0109 09:48:50.907348     352 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0109 09:48:51.006192     352 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0109 09:48:51.126610     352 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0109 09:48:51.224556     352 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0109 09:48:51.244110     352 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0109 09:48:51.343801     352 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0109 09:48:51.552475     352 start.go:528] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0109 09:48:51.552587     352 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0109 09:48:51.556214     352 start.go:549] Will wait 60s for crictl version
I0109 09:48:51.556252     352 ssh_runner.go:195] Run: which crictl
I0109 09:48:51.559845     352 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0109 09:48:51.679674     352 start.go:565] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  23.0.2
RuntimeApiVersion:  v1alpha2
I0109 09:48:51.679723     352 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0109 09:48:51.766666     352 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0109 09:48:51.791554     352 out.go:204] 🐳  Preparing Kubernetes v1.26.3 on Docker 23.0.2 ...
I0109 09:48:51.791689     352 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0109 09:48:51.860901     352 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0109 09:48:51.864220     352 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0109 09:48:51.873631     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0109 09:48:51.942815     352 preload.go:132] Checking if preload exists for k8s version v1.26.3 and runtime docker
I0109 09:48:51.942854     352 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0109 09:48:51.967553     352 docker.go:639] Got preloaded images: -- stdout --
fajarsujailoyato/go-hello-world:v2
fajarsujailoyato/go-hello-world:v1
fajarsujai/go-hello-world:v1
fajarsujai/go-hello-world:lts
fajarsujai/go-hello-world:latest
bitnami/wordpress:6.4.1-debian-11-r1
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/kube-apiserver:v1.26.3
registry.k8s.io/kube-controller-manager:v1.26.3
registry.k8s.io/kube-scheduler:v1.26.3
registry.k8s.io/kube-proxy:v1.26.3
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5
polinux/stress:latest
nginx:1.14.2
vish/stress:latest
nginx:1.9.1

-- /stdout --
I0109 09:48:51.967584     352 docker.go:569] Images already preloaded, skipping extraction
I0109 09:48:51.967650     352 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0109 09:48:51.986333     352 docker.go:639] Got preloaded images: -- stdout --
fajarsujailoyato/go-hello-world:v2
fajarsujailoyato/go-hello-world:v1
fajarsujai/go-hello-world:v1
fajarsujai/go-hello-world:lts
fajarsujai/go-hello-world:latest
bitnami/wordpress:6.4.1-debian-11-r1
registry.k8s.io/metrics-server/metrics-server:<none>
registry.k8s.io/kube-apiserver:v1.26.3
registry.k8s.io/kube-scheduler:v1.26.3
registry.k8s.io/kube-controller-manager:v1.26.3
registry.k8s.io/kube-proxy:v1.26.3
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
gcr.io/k8s-minikube/storage-provisioner:v5
polinux/stress:latest
nginx:1.14.2
vish/stress:latest
nginx:1.9.1

-- /stdout --
I0109 09:48:51.986347     352 cache_images.go:84] Images are preloaded, skipping loading
I0109 09:48:51.986436     352 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0109 09:48:52.010872     352 cni.go:84] Creating CNI manager for ""
I0109 09:48:52.010884     352 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0109 09:48:52.010898     352 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0109 09:48:52.010909     352 kubeadm.go:172] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.26.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m]}
I0109 09:48:52.011005     352 kubeadm.go:177] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.26.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0109 09:48:52.011047     352 kubeadm.go:968] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.26.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0109 09:48:52.011102     352 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.26.3
I0109 09:48:52.022770     352 binaries.go:44] Found k8s binaries, skipping transfer
I0109 09:48:52.022826     352 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0109 09:48:52.031981     352 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0109 09:48:52.048211     352 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0109 09:48:52.061977     352 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2084 bytes)
I0109 09:48:52.076244     352 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0109 09:48:52.079691     352 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0109 09:48:52.089784     352 certs.go:56] Setting up /home/fajarsujai/.minikube/profiles/minikube for IP: 192.168.49.2
I0109 09:48:52.089801     352 certs.go:186] acquiring lock for shared ca certs: {Name:mk655d35f1a6f9127b0c93224b8c4cc3a3a76829 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0109 09:48:52.089905     352 certs.go:195] skipping minikubeCA CA generation: /home/fajarsujai/.minikube/ca.key
I0109 09:48:52.090508     352 certs.go:195] skipping proxyClientCA CA generation: /home/fajarsujai/.minikube/proxy-client-ca.key
I0109 09:48:52.090553     352 certs.go:311] skipping minikube-user signed cert generation: /home/fajarsujai/.minikube/profiles/minikube/client.key
I0109 09:48:52.090841     352 certs.go:311] skipping minikube signed cert generation: /home/fajarsujai/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0109 09:48:52.091079     352 certs.go:311] skipping aggregator signed cert generation: /home/fajarsujai/.minikube/profiles/minikube/proxy-client.key
I0109 09:48:52.091144     352 certs.go:401] found cert: /home/fajarsujai/.minikube/certs/home/fajarsujai/.minikube/certs/ca-key.pem (1675 bytes)
I0109 09:48:52.091161     352 certs.go:401] found cert: /home/fajarsujai/.minikube/certs/home/fajarsujai/.minikube/certs/ca.pem (1086 bytes)
I0109 09:48:52.091173     352 certs.go:401] found cert: /home/fajarsujai/.minikube/certs/home/fajarsujai/.minikube/certs/cert.pem (1131 bytes)
I0109 09:48:52.091185     352 certs.go:401] found cert: /home/fajarsujai/.minikube/certs/home/fajarsujai/.minikube/certs/key.pem (1675 bytes)
I0109 09:48:52.091572     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0109 09:48:52.110780     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0109 09:48:52.130210     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0109 09:48:52.151551     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0109 09:48:52.171846     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0109 09:48:52.191916     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0109 09:48:52.214707     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0109 09:48:52.234489     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0109 09:48:52.255112     352 ssh_runner.go:362] scp /home/fajarsujai/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0109 09:48:52.274669     352 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0109 09:48:52.290586     352 ssh_runner.go:195] Run: openssl version
I0109 09:48:52.298053     352 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0109 09:48:52.307113     352 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0109 09:48:52.310514     352 certs.go:444] hashing: -rw-r--r-- 1 root root 1111 Nov 12 03:09 /usr/share/ca-certificates/minikubeCA.pem
I0109 09:48:52.310551     352 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0109 09:48:52.315380     352 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0109 09:48:52.323171     352 kubeadm.go:401] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.39 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/fajarsujai:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0109 09:48:52.323272     352 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0109 09:48:52.340269     352 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0109 09:48:52.348292     352 kubeadm.go:416] found existing configuration files, will attempt cluster restart
I0109 09:48:52.348306     352 kubeadm.go:633] restartCluster start
I0109 09:48:52.348377     352 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0109 09:48:52.356916     352 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0109 09:48:52.356979     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0109 09:48:52.420983     352 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:57980"
I0109 09:48:52.420994     352 kubeconfig.go:135] verify returned: got: 127.0.0.1:57980, want: 127.0.0.1:58380
I0109 09:48:52.421553     352 lock.go:35] WriteFile acquiring /home/fajarsujai/.kube/config: {Name:mk2b94e395e9bcddf1bdd5776da7f9c7328795b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0109 09:48:52.428878     352 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0109 09:48:52.437079     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:52.437126     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:52.451054     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:52.951214     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:52.951327     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:52.965318     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:53.451253     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:53.451355     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:53.465872     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:53.951950     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:53.952031     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:53.967018     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:54.451528     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:54.451603     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:54.461536     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:54.951686     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:54.951748     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:54.963563     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:55.451466     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:55.451514     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:55.466412     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:55.951546     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:55.951594     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:55.961965     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:56.452257     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:56.452329     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:56.466664     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:56.951222     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:56.951276     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:56.962836     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:57.451192     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:57.451293     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:57.471177     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:57.952220     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:57.952295     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:57.962518     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:58.451287     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:58.451367     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:58.462832     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:58.952038     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:58.952385     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:58.974445     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:59.452108     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:59.452186     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:59.472234     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:48:59.951843     352 api_server.go:165] Checking apiserver status ...
I0109 09:48:59.951915     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:48:59.985237     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:00.451768     352 api_server.go:165] Checking apiserver status ...
I0109 09:49:00.451853     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:49:00.496620     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:00.952170     352 api_server.go:165] Checking apiserver status ...
I0109 09:49:00.952272     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:49:00.967933     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:01.451734     352 api_server.go:165] Checking apiserver status ...
I0109 09:49:01.451847     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:49:01.473755     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:01.952015     352 api_server.go:165] Checking apiserver status ...
I0109 09:49:01.952147     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:49:01.968149     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:02.452003     352 api_server.go:165] Checking apiserver status ...
I0109 09:49:02.452099     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:49:02.468023     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:02.468036     352 api_server.go:165] Checking apiserver status ...
I0109 09:49:02.468106     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0109 09:49:02.493412     352 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0109 09:49:02.493433     352 kubeadm.go:608] needs reconfigure: apiserver error: timed out waiting for the condition
I0109 09:49:02.493442     352 kubeadm.go:1120] stopping kube-system containers ...
I0109 09:49:02.493536     352 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0109 09:49:02.565829     352 docker.go:465] Stopping containers: [e49cd0cc7f95 2850289daa6b 8b066ae9a92f 3b3a1283a112 36e63978f59b c11f48772af5 0f24a441a5d2 965a0100cc75 63d5ef1dd829 4d3f61a51f02 c5ed3e194fa1 cb712b03f2fe f22685ea40e1 a838bde35dc2 bafe6890a753 6f1b3972c6e4 24952554db75 c25ff6cea61f 363da7ec7dd2 2e0a56505619 c19f039dde4d 617c786bea07 9044efa6cc8f cc0c37966b04 214cf8c54b16 81cbdd2db751 c4a2b667c3ae 20b910744b3d c818e8bf0c51 7175d12f5b69 c96a19f38cde]
I0109 09:49:02.565942     352 ssh_runner.go:195] Run: docker stop e49cd0cc7f95 2850289daa6b 8b066ae9a92f 3b3a1283a112 36e63978f59b c11f48772af5 0f24a441a5d2 965a0100cc75 63d5ef1dd829 4d3f61a51f02 c5ed3e194fa1 cb712b03f2fe f22685ea40e1 a838bde35dc2 bafe6890a753 6f1b3972c6e4 24952554db75 c25ff6cea61f 363da7ec7dd2 2e0a56505619 c19f039dde4d 617c786bea07 9044efa6cc8f cc0c37966b04 214cf8c54b16 81cbdd2db751 c4a2b667c3ae 20b910744b3d c818e8bf0c51 7175d12f5b69 c96a19f38cde
I0109 09:49:02.630701     352 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0109 09:49:02.655082     352 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0109 09:49:02.673475     352 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Nov 12 03:13 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Jan  8 06:46 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Nov 12 03:14 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Jan  8 06:46 /etc/kubernetes/scheduler.conf

I0109 09:49:02.673547     352 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0109 09:49:02.706386     352 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0109 09:49:02.755349     352 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0109 09:49:02.787097     352 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0109 09:49:02.787182     352 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0109 09:49:02.814684     352 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0109 09:49:02.858424     352 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0109 09:49:02.858492     352 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0109 09:49:02.877597     352 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0109 09:49:02.896879     352 kubeadm.go:710] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0109 09:49:02.896895     352 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0109 09:49:03.344539     352 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0109 09:49:04.581887     352 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.237329813s)
I0109 09:49:04.581904     352 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0109 09:49:04.780280     352 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0109 09:49:04.834264     352 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0109 09:49:04.891833     352 api_server.go:51] waiting for apiserver process to appear ...
I0109 09:49:04.891880     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:05.402465     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:05.902300     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:06.402553     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:06.902459     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:07.402572     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:07.903038     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:08.402330     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:08.962937     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:08.983896     352 api_server.go:71] duration metric: took 4.092058027s to wait for apiserver process to appear ...
I0109 09:49:08.984019     352 api_server.go:87] waiting for apiserver healthz status ...
I0109 09:49:08.984034     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:08.999998     352 api_server.go:268] stopped: https://127.0.0.1:58380/healthz: Get "https://127.0.0.1:58380/healthz": EOF
I0109 09:49:09.501409     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:09.504903     352 api_server.go:268] stopped: https://127.0.0.1:58380/healthz: Get "https://127.0.0.1:58380/healthz": EOF
I0109 09:49:10.001133     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:13.655954     352 api_server.go:278] https://127.0.0.1:58380/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0109 09:49:13.655974     352 api_server.go:102] status: https://127.0.0.1:58380/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0109 09:49:14.000767     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:14.006444     352 api_server.go:278] https://127.0.0.1:58380/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0109 09:49:14.006460     352 api_server.go:102] status: https://127.0.0.1:58380/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0109 09:49:14.501218     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:14.509122     352 api_server.go:278] https://127.0.0.1:58380/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0109 09:49:14.509146     352 api_server.go:102] status: https://127.0.0.1:58380/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0109 09:49:15.000843     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:15.005786     352 api_server.go:278] https://127.0.0.1:58380/healthz returned 200:
ok
I0109 09:49:15.014454     352 api_server.go:140] control plane version: v1.26.3
I0109 09:49:15.014481     352 api_server.go:130] duration metric: took 6.03045484s to wait for apiserver health ...
I0109 09:49:15.014488     352 cni.go:84] Creating CNI manager for ""
I0109 09:49:15.014515     352 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0109 09:49:15.016518     352 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0109 09:49:15.018142     352 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0109 09:49:15.026651     352 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0109 09:49:15.042356     352 system_pods.go:43] waiting for kube-system pods to appear ...
I0109 09:49:15.052739     352 system_pods.go:59] 8 kube-system pods found
I0109 09:49:15.052752     352 system_pods.go:61] "coredns-787d4945fb-hkk6k" [38c538db-7b29-4733-953c-05e43155e788] Running
I0109 09:49:15.052755     352 system_pods.go:61] "etcd-minikube" [9bb092b2-0e9f-44bc-988d-fc2a49c65241] Running
I0109 09:49:15.052758     352 system_pods.go:61] "kube-apiserver-minikube" [f8165383-f9ba-4c42-bc28-7d3eb05c5e2c] Running
I0109 09:49:15.052761     352 system_pods.go:61] "kube-controller-manager-minikube" [d36f9977-a88a-48b3-8916-7f28005ec231] Running
I0109 09:49:15.052764     352 system_pods.go:61] "kube-proxy-hchxp" [9979396f-9fc1-4605-8593-a24ca4a1cdaa] Running
I0109 09:49:15.052766     352 system_pods.go:61] "kube-scheduler-minikube" [03b546ae-9937-42a8-88dd-772fbcfceb85] Running
I0109 09:49:15.052769     352 system_pods.go:61] "metrics-server-6588d95b98-kdjdj" [b021efb1-30c1-48f9-bd77-57ac228c5392] Running
I0109 09:49:15.052807     352 system_pods.go:61] "storage-provisioner" [78fb396a-fa7e-471b-b408-eaf40924d80e] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0109 09:49:15.052812     352 system_pods.go:74] duration metric: took 10.44837ms to wait for pod list to return data ...
I0109 09:49:15.052817     352 node_conditions.go:102] verifying NodePressure condition ...
I0109 09:49:15.056941     352 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0109 09:49:15.056951     352 node_conditions.go:123] node cpu capacity is 8
I0109 09:49:15.056958     352 node_conditions.go:105] duration metric: took 4.138738ms to run NodePressure ...
I0109 09:49:15.056990     352 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0109 09:49:15.230013     352 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0109 09:49:15.236557     352 ops.go:34] apiserver oom_adj: -16
I0109 09:49:15.236573     352 kubeadm.go:637] restartCluster took 22.888223374s
I0109 09:49:15.236580     352 kubeadm.go:403] StartCluster complete in 22.913378787s
I0109 09:49:15.236656     352 settings.go:142] acquiring lock: {Name:mk8e9ccc7c885c47e5aa8be768b53050b167fde0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0109 09:49:15.236794     352 settings.go:150] Updating kubeconfig:  /home/fajarsujai/.kube/config
I0109 09:49:15.237538     352 lock.go:35] WriteFile acquiring /home/fajarsujai/.kube/config: {Name:mk2b94e395e9bcddf1bdd5776da7f9c7328795b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0109 09:49:15.237863     352 addons.go:496] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0109 09:49:15.237951     352 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0109 09:49:15.237966     352 addons.go:66] Setting storage-provisioner=true in profile "minikube"
I0109 09:49:15.237983     352 addons.go:228] Setting addon storage-provisioner=true in "minikube"
W0109 09:49:15.237988     352 addons.go:237] addon storage-provisioner should already be in state true
I0109 09:49:15.237997     352 addons.go:66] Setting default-storageclass=true in profile "minikube"
I0109 09:49:15.238026     352 host.go:66] Checking if "minikube" exists ...
I0109 09:49:15.238025     352 addons.go:66] Setting dashboard=true in profile "minikube"
I0109 09:49:15.238024     352 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0109 09:49:15.238029     352 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.26.3
I0109 09:49:15.238034     352 addons.go:228] Setting addon dashboard=true in "minikube"
W0109 09:49:15.238039     352 addons.go:237] addon dashboard should already be in state true
I0109 09:49:15.238038     352 addons.go:66] Setting metrics-server=true in profile "minikube"
I0109 09:49:15.238058     352 addons.go:228] Setting addon metrics-server=true in "minikube"
W0109 09:49:15.238063     352 addons.go:237] addon metrics-server should already be in state true
I0109 09:49:15.238095     352 host.go:66] Checking if "minikube" exists ...
I0109 09:49:15.238145     352 host.go:66] Checking if "minikube" exists ...
I0109 09:49:15.238401     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:49:15.238397     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:49:15.238478     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:49:15.238505     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:49:15.242665     352 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0109 09:49:15.242713     352 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.26.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0109 09:49:15.249763     352 out.go:177] 🔎  Verifying Kubernetes components...
I0109 09:49:15.252515     352 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0109 09:49:15.401566     352 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0109 09:49:15.405999     352 out.go:177]     ▪ Using image registry.k8s.io/metrics-server/metrics-server:v0.6.3
I0109 09:49:15.408534     352 addons.go:420] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0109 09:49:15.408548     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0109 09:49:15.410780     352 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0109 09:49:15.408645     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:49:15.413497     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0109 09:49:15.414025     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0109 09:49:15.414081     352 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0109 09:49:15.414111     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:49:15.413548     352 addons.go:228] Setting addon default-storageclass=true in "minikube"
W0109 09:49:15.416736     352 addons.go:237] addon default-storageclass should already be in state true
I0109 09:49:15.416771     352 host.go:66] Checking if "minikube" exists ...
I0109 09:49:15.416784     352 addons.go:420] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0109 09:49:15.416795     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0109 09:49:15.416862     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:49:15.417327     352 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0109 09:49:15.551474     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:49:15.560963     352 addons.go:420] installing /etc/kubernetes/addons/storageclass.yaml
I0109 09:49:15.560977     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0109 09:49:15.561067     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0109 09:49:15.584441     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:49:15.590544     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:49:15.665980     352 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58376 SSHKeyPath:/home/fajarsujai/.minikube/machines/minikube/id_rsa Username:docker}
I0109 09:49:16.107021     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0109 09:49:16.107049     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0109 09:49:16.109125     352 addons.go:420] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0109 09:49:16.109148     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0109 09:49:16.202071     352 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0109 09:49:16.306120     352 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0109 09:49:16.405272     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0109 09:49:16.405296     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0109 09:49:16.417882     352 addons.go:420] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0109 09:49:16.417915     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0109 09:49:16.814642     352 addons.go:420] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0109 09:49:16.814670     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0109 09:49:16.816991     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0109 09:49:16.817015     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0109 09:49:17.303640     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0109 09:49:17.303664     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0109 09:49:17.422582     352 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0109 09:49:18.021894     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-role.yaml
I0109 09:49:18.021911     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0109 09:49:18.401044     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0109 09:49:18.401059     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0109 09:49:18.702476     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0109 09:49:18.702492     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0109 09:49:18.922005     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0109 09:49:18.922020     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0109 09:49:19.321600     352 addons.go:420] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0109 09:49:19.321664     352 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0109 09:49:19.708898     352 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0109 09:49:21.700381     352 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (6.447825878s)
I0109 09:49:21.700527     352 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0109 09:49:21.700849     352 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (6.462850361s)
I0109 09:49:21.700976     352 start.go:889] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0109 09:49:21.810550     352 api_server.go:51] waiting for apiserver process to appear ...
I0109 09:49:21.810610     352 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0109 09:49:24.203076     352 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (8.000964343s)
I0109 09:49:24.203246     352 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (7.897104622s)
I0109 09:49:24.203342     352 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (6.78074086s)
I0109 09:49:24.203355     352 addons.go:464] Verifying addon metrics-server=true in "minikube"
I0109 09:49:24.423661     352 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.26.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (4.714712769s)
I0109 09:49:24.429460     352 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0109 09:49:24.424876     352 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.614192304s)
I0109 09:49:24.433860     352 api_server.go:71] duration metric: took 9.191107328s to wait for apiserver process to appear ...
I0109 09:49:24.433878     352 api_server.go:87] waiting for apiserver healthz status ...
I0109 09:49:24.433898     352 api_server.go:252] Checking apiserver healthz at https://127.0.0.1:58380/healthz ...
I0109 09:49:24.437237     352 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, metrics-server, dashboard
I0109 09:49:24.439322     352 addons.go:499] enable addons completed in 9.201460086s: enabled=[storage-provisioner default-storageclass metrics-server dashboard]
I0109 09:49:24.505471     352 api_server.go:278] https://127.0.0.1:58380/healthz returned 200:
ok
I0109 09:49:24.508117     352 api_server.go:140] control plane version: v1.26.3
I0109 09:49:24.508136     352 api_server.go:130] duration metric: took 74.252583ms to wait for apiserver health ...
I0109 09:49:24.508144     352 system_pods.go:43] waiting for kube-system pods to appear ...
I0109 09:49:24.520962     352 system_pods.go:59] 8 kube-system pods found
I0109 09:49:24.520990     352 system_pods.go:61] "coredns-787d4945fb-hkk6k" [38c538db-7b29-4733-953c-05e43155e788] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0109 09:49:24.520998     352 system_pods.go:61] "etcd-minikube" [9bb092b2-0e9f-44bc-988d-fc2a49c65241] Running
I0109 09:49:24.521006     352 system_pods.go:61] "kube-apiserver-minikube" [f8165383-f9ba-4c42-bc28-7d3eb05c5e2c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0109 09:49:24.521011     352 system_pods.go:61] "kube-controller-manager-minikube" [d36f9977-a88a-48b3-8916-7f28005ec231] Running
I0109 09:49:24.521018     352 system_pods.go:61] "kube-proxy-hchxp" [9979396f-9fc1-4605-8593-a24ca4a1cdaa] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0109 09:49:24.521024     352 system_pods.go:61] "kube-scheduler-minikube" [03b546ae-9937-42a8-88dd-772fbcfceb85] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0109 09:49:24.521031     352 system_pods.go:61] "metrics-server-6588d95b98-kdjdj" [b021efb1-30c1-48f9-bd77-57ac228c5392] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0109 09:49:24.521039     352 system_pods.go:61] "storage-provisioner" [78fb396a-fa7e-471b-b408-eaf40924d80e] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0109 09:49:24.521046     352 system_pods.go:74] duration metric: took 12.8959ms to wait for pod list to return data ...
I0109 09:49:24.521059     352 kubeadm.go:578] duration metric: took 9.278313015s to wait for : map[apiserver:true system_pods:true] ...
I0109 09:49:24.521077     352 node_conditions.go:102] verifying NodePressure condition ...
I0109 09:49:24.531553     352 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0109 09:49:24.531572     352 node_conditions.go:123] node cpu capacity is 8
I0109 09:49:24.531586     352 node_conditions.go:105] duration metric: took 10.504242ms to run NodePressure ...
I0109 09:49:24.531600     352 start.go:228] waiting for startup goroutines ...
I0109 09:49:24.531609     352 start.go:233] waiting for cluster config update ...
I0109 09:49:24.531622     352 start.go:242] writing updated cluster config ...
I0109 09:49:24.531989     352 ssh_runner.go:195] Run: rm -f paused
I0109 09:49:24.695503     352 start.go:568] kubectl: 1.25.4, cluster: 1.26.3 (minor skew: 1)
I0109 09:49:24.697726     352 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
-- Logs begin at Tue 2024-01-09 02:48:40 UTC, end at Tue 2024-01-09 08:33:09 UTC. --
Jan 09 07:59:01 minikube dockerd[1220]: time="2024-01-09T07:59:01.426099308Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 07:59:01 minikube dockerd[1220]: time="2024-01-09T07:59:01.440839538Z" level=error msg="61e848017477331f6b9b1392a049787359a9525e7a80236868fcf4cb4102e373 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:03:25 minikube dockerd[1220]: time="2024-01-09T08:03:25.412906793Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:03:25 minikube dockerd[1220]: time="2024-01-09T08:03:25.413042594Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:03:25 minikube dockerd[1220]: time="2024-01-09T08:03:25.431098696Z" level=error msg="925fda54e60a936c818f92b297b15f6a4121e80942126a92cd7cefca49664af6 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:04:10 minikube dockerd[1220]: time="2024-01-09T08:04:10.386594425Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:04:10 minikube dockerd[1220]: time="2024-01-09T08:04:10.386632954Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:04:10 minikube dockerd[1220]: time="2024-01-09T08:04:10.403210684Z" level=error msg="c160b5d01545bb34cae69a125b2cc4444d90c6dba2f00868e62c273a194b5d88 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:08:36 minikube dockerd[1220]: time="2024-01-09T08:08:36.429964183Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:08:36 minikube dockerd[1220]: time="2024-01-09T08:08:36.430061497Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:08:36 minikube dockerd[1220]: time="2024-01-09T08:08:36.445133532Z" level=error msg="eeffb59691b7d7e1aaea2bb5936085dba373f33a6e0f1f0b35d765ce383355d2 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:09:12 minikube dockerd[1220]: time="2024-01-09T08:09:12.405862000Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:09:12 minikube dockerd[1220]: time="2024-01-09T08:09:12.406015544Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:09:12 minikube dockerd[1220]: time="2024-01-09T08:09:12.427811481Z" level=error msg="6139dec2493e1132f2017ca2c554d4ae64429fde09434e4cb456af50cf5fa086 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:13:39 minikube dockerd[1220]: time="2024-01-09T08:13:39.580605246Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:13:39 minikube dockerd[1220]: time="2024-01-09T08:13:39.580941552Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:13:39 minikube dockerd[1220]: time="2024-01-09T08:13:39.623727432Z" level=error msg="e155fee8206dfd2aca462271c96150c16782fd738aa0cf2e626168c01ecab151 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:14:16 minikube dockerd[1220]: time="2024-01-09T08:14:16.443179882Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:14:16 minikube dockerd[1220]: time="2024-01-09T08:14:16.443267974Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:14:16 minikube dockerd[1220]: time="2024-01-09T08:14:16.465885899Z" level=error msg="fa54c59bbe7ba554ddfa4c4db91b1394b54c6d2fb613e1907a6f194f5e93cdfd cleanup: failed to delete container from containerd: no such container"
Jan 09 08:18:53 minikube dockerd[1220]: time="2024-01-09T08:18:53.429423228Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:18:53 minikube dockerd[1220]: time="2024-01-09T08:18:53.430048432Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:18:53 minikube dockerd[1220]: time="2024-01-09T08:18:53.484747325Z" level=error msg="a814cbf56682f4abc698874fd1d53c0c8a453b89f3cfd140a734c710f54d2d63 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:19:28 minikube dockerd[1220]: time="2024-01-09T08:19:28.409155295Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:19:28 minikube dockerd[1220]: time="2024-01-09T08:19:28.409295318Z" level=error msg="stream copy error: reading from a closed fifo"
Jan 09 08:19:28 minikube dockerd[1220]: time="2024-01-09T08:19:28.426584136Z" level=error msg="e38645ada4ab4d1492d479f41a2a7559c872b5036a7a7c122ebf9d6a63339cd7 cleanup: failed to delete container from containerd: no such container"
Jan 09 08:20:35 minikube dockerd[1220]: time="2024-01-09T08:20:35.151405658Z" level=info msg="ignoring event" container=72fb48baa4a800cd91b373c3c0c0f06e747f67faf79217f8b73165245f6eafe6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:20:35 minikube cri-dockerd[1489]: time="2024-01-09T08:20:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09ae774d711f30186c1ddafa233d041aae1ede8d229899ad14cadce7da501fe4/resolv.conf as [nameserver 10.96.0.10 search develop-ghw-sc-vol.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 09 08:20:53 minikube cri-dockerd[1489]: time="2024-01-09T08:20:53Z" level=info msg="Pulling image fajarsujailoyato/go-hello-world:v3: 499c4b4f29ca: Downloading [=========================================>         ]  16.21MB/19.46MB"
Jan 09 08:20:56 minikube cri-dockerd[1489]: time="2024-01-09T08:20:56Z" level=info msg="Stop pulling image fajarsujailoyato/go-hello-world:v3: Status: Downloaded newer image for fajarsujailoyato/go-hello-world:v3"
Jan 09 08:20:56 minikube dockerd[1220]: time="2024-01-09T08:20:56.672734418Z" level=info msg="ignoring event" container=272fb19041bdab1d9c8fbf93e30b03b9f7d0c970e460d4081d36181d06d893da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:20:57 minikube dockerd[1220]: time="2024-01-09T08:20:57.601718880Z" level=info msg="ignoring event" container=ba3361abee86a98505f08ece558274c9f1703e064d6ee4d885fac959e4e98bea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:21:12 minikube dockerd[1220]: time="2024-01-09T08:21:12.433796853Z" level=info msg="ignoring event" container=d25ac3253b5770ca33a89b8c9c465179d2a2a65079e22eef81b072814459efcd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:21:15 minikube dockerd[1220]: time="2024-01-09T08:21:15.023088488Z" level=info msg="ignoring event" container=09ae774d711f30186c1ddafa233d041aae1ede8d229899ad14cadce7da501fe4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:21:15 minikube dockerd[1220]: time="2024-01-09T08:21:15.126208244Z" level=info msg="ignoring event" container=9626390ecc65282e12afe085f81ee18ec7239765e3bca01ba5578a5d48ffbf6e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:21:30 minikube cri-dockerd[1489]: time="2024-01-09T08:21:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e1860bc9e066c1c95aca6ce972471159273c482cc8d955f3106f38fb96468951/resolv.conf as [nameserver 10.96.0.10 search develop-ghw-sc-vol.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 09 08:21:31 minikube dockerd[1220]: time="2024-01-09T08:21:31.022967708Z" level=info msg="ignoring event" container=dff87ebfedef49b155575dd911412de5d61d901ef6b044db05404dc3a323efa2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:21:32 minikube dockerd[1220]: time="2024-01-09T08:21:32.258173159Z" level=info msg="ignoring event" container=de3064b7088a457f5e1bd6b043c23ba07fd46c92fa559208894c0af55672e16a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:21:45 minikube dockerd[1220]: time="2024-01-09T08:21:45.518166425Z" level=info msg="ignoring event" container=1d2975b220f53c2955926518146108815a724ab6b768dd783bc5bde88d72643d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:15 minikube cri-dockerd[1489]: time="2024-01-09T08:22:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0cf254beece4989df2a06410248eb113829b91b97409b0bd7a60786106511dc3/resolv.conf as [nameserver 10.96.0.10 search develop-ghw-sc-vol.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 09 08:22:16 minikube dockerd[1220]: time="2024-01-09T08:22:16.197805697Z" level=info msg="ignoring event" container=a8d1ac695fc826f8a7c67a8b4f3dddc45d982bb19a6934ad5f9c6252c9d3e17e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:16 minikube dockerd[1220]: time="2024-01-09T08:22:16.407859623Z" level=info msg="ignoring event" container=85c21424634744391a5b12b7768348ce41ea681953f6a3142cf2771345fa57a6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:16 minikube dockerd[1220]: time="2024-01-09T08:22:16.977149314Z" level=info msg="ignoring event" container=2a0c0c5d928c7e555ec8d9657cbbc73c0b559356637141b42e387362434a150f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:30 minikube dockerd[1220]: time="2024-01-09T08:22:30.435610571Z" level=info msg="ignoring event" container=af0199ffe733cfabc50eebfe138c8e2676674549c445c1a032ab6a56282114b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:34 minikube dockerd[1220]: time="2024-01-09T08:22:34.135079015Z" level=info msg="ignoring event" container=e1860bc9e066c1c95aca6ce972471159273c482cc8d955f3106f38fb96468951 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:34 minikube dockerd[1220]: time="2024-01-09T08:22:34.194513583Z" level=info msg="ignoring event" container=0cf254beece4989df2a06410248eb113829b91b97409b0bd7a60786106511dc3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:43 minikube cri-dockerd[1489]: time="2024-01-09T08:22:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ba3eead3dd68673fc09c429b691715d6681bad517c70423d453042cfd64f7795/resolv.conf as [nameserver 10.96.0.10 search develop-ghw-sc-vol.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 09 08:22:43 minikube dockerd[1220]: time="2024-01-09T08:22:43.755517574Z" level=info msg="ignoring event" container=b6527ff217204a419a270af0b70a47bbafdec965aaf162e6dba4b03d52977d8b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:44 minikube dockerd[1220]: time="2024-01-09T08:22:44.635171198Z" level=info msg="ignoring event" container=8c2ef1674a7df297e7b67d8117ff590d0770f7d365b40b40e68bff3f78d08cb9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:22:58 minikube dockerd[1220]: time="2024-01-09T08:22:58.426466783Z" level=info msg="ignoring event" container=bc9dd36e3d455751527bdf67db93e85022d31086ad4fee6223b6cecf28f45ed5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:23:25 minikube dockerd[1220]: time="2024-01-09T08:23:25.445700755Z" level=info msg="ignoring event" container=0d49872eecaaac09d16621d1d1817df52eb96c542c97cf50526f3c84d5944396 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:24:16 minikube dockerd[1220]: time="2024-01-09T08:24:16.446895450Z" level=info msg="ignoring event" container=b2fa3ba636791b24d3d3e97260f9ddd029a531eacb04c1e3c15e8d550c1537d4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:25:38 minikube dockerd[1220]: time="2024-01-09T08:25:38.540756481Z" level=info msg="ignoring event" container=aec339df2f6070041d9a3ec386eb8a09bbdce5faa0049afd00b83cf1883e06d6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:28:26 minikube dockerd[1220]: time="2024-01-09T08:28:26.475034281Z" level=info msg="ignoring event" container=e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:31:51 minikube cri-dockerd[1489]: time="2024-01-09T08:31:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c32500b97015e4f752d1a1dacdcef8e17f25b8106acaa3752b37e62b652f27d/resolv.conf as [nameserver 10.96.0.10 search develop-ghw-sc-vol.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 09 08:32:06 minikube cri-dockerd[1489]: time="2024-01-09T08:32:06Z" level=info msg="Stop pulling image fajarsujailoyato/go-hello-world:v4: Status: Downloaded newer image for fajarsujailoyato/go-hello-world:v4"
Jan 09 08:32:07 minikube dockerd[1220]: time="2024-01-09T08:32:07.888428339Z" level=info msg="ignoring event" container=ba3eead3dd68673fc09c429b691715d6681bad517c70423d453042cfd64f7795 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:32:09 minikube dockerd[1220]: time="2024-01-09T08:32:09.455346524Z" level=info msg="ignoring event" container=f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:32:09 minikube dockerd[1220]: time="2024-01-09T08:32:09.838327929Z" level=info msg="ignoring event" container=6c32500b97015e4f752d1a1dacdcef8e17f25b8106acaa3752b37e62b652f27d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 09 08:32:12 minikube cri-dockerd[1489]: time="2024-01-09T08:32:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1de614f96b56d932c808e9aa290558be7bc2512f185415ef4e2f2e8d2efd9ecd/resolv.conf as [nameserver 10.96.0.10 search develop-ghw-sc-vol.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID
b8e62789e2cec       b833f87d5bb48       58 seconds ago      Running             ghw-container-vol           0                   1de614f96b56d
7d550e7a3b84b       94ec7e53edfc7       6 hours ago         Running             nginx                       3                   3250f24680f4a
ce637b62183d0       07655ddf2eebe       6 hours ago         Running             kubernetes-dashboard        42                  06498d476a21b
421a9bc2b3c92       817bbe3f2e517       6 hours ago         Running             metrics-server              19                  d0dd9f28d27f8
0ad3bc41b9f8d       94ec7e53edfc7       6 hours ago         Running             nginx                       3                   475c0e308b92e
3273521f91f46       115053965e86b       6 hours ago         Running             dashboard-metrics-scraper   15                  cad841afe72ef
797d855cbf5e8       6f40dd6d01db7       6 hours ago         Running             ghw-container               4                   efaaaa630a8ca
fe680e2c526eb       92ed2bec97a63       6 hours ago         Running             kube-proxy                  18                  b4d410d6e4565
f3525f9ce9deb       5185b96f0becf       6 hours ago         Running             coredns                     18                  8df6bf5915dee
d519c6e3adb64       94ec7e53edfc7       6 hours ago         Running             nginx                       3                   4f2a2241d9571
829b625e6af54       6e38f40d628db       6 hours ago         Running             storage-provisioner         124                 d960f501ad727
1863ee8ebe0d8       5a79047369329       6 hours ago         Running             kube-scheduler              18                  c0d6417c02762
bdd4d6518985d       1d9b3cbae03ce       6 hours ago         Running             kube-apiserver              18                  615dbfc5daa0b
0bb5326a48d62       fce326961ae2d       6 hours ago         Running             etcd                        19                  68b6afd17c48f
ef434ffd62fb9       ce8c2293ef09c       6 hours ago         Running             kube-controller-manager     20                  86e103e5e3566
e04f1478f47f7       6f40dd6d01db7       20 hours ago        Exited              ghw-container               3                   259ab5d3b1f8f
e49cd0cc7f956       6e38f40d628db       26 hours ago        Exited              storage-provisioner         123                 c11f48772af5e
1ecf6ac323e29       94ec7e53edfc7       26 hours ago        Exited              nginx                       2                   0de84d4edc640
c2dd7a23742b0       94ec7e53edfc7       26 hours ago        Exited              nginx                       2                   7be668faf39a8
0c72c68063b2a       07655ddf2eebe       26 hours ago        Exited              kubernetes-dashboard        41                  db5e154751277
efe21abbc6c09       115053965e86b       26 hours ago        Exited              dashboard-metrics-scraper   14                  5cfd6ae05d63f
7f6a67fcc9662       94ec7e53edfc7       26 hours ago        Exited              nginx                       2                   635c0ebc3c06c
2850289daa6b2       5185b96f0becf       26 hours ago        Exited              coredns                     17                  965a0100cc753
8b066ae9a92f1       817bbe3f2e517       26 hours ago        Exited              metrics-server              18                  63d5ef1dd8292
36e63978f59bd       92ed2bec97a63       26 hours ago        Exited              kube-proxy                  17                  0f24a441a5d23
4d3f61a51f02f       5a79047369329       26 hours ago        Exited              kube-scheduler              17                  a838bde35dc2c
c5ed3e194fa1f       ce8c2293ef09c       26 hours ago        Exited              kube-controller-manager     19                  bafe6890a753c
cb712b03f2fea       fce326961ae2d       26 hours ago        Exited              etcd                        18                  6f1b3972c6e46
f22685ea40e11       1d9b3cbae03ce       26 hours ago        Exited              kube-apiserver              17                  24952554db75d


==> coredns [2850289daa6b] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:60292 - 8704 "HINFO IN 2967571250467194404.2986636235141695495. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.097517941s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [f3525f9ce9de] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:39691 - 51077 "HINFO IN 1237386219991895238.3008631697666746492. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.030272807s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=08896fd1dc362c097c925146c4a0d0dac715ace0
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_12T10_14_12_0700
                    minikube.k8s.io/version=v1.30.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 12 Nov 2023 03:14:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 09 Jan 2024 08:33:04 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 09 Jan 2024 08:32:21 +0000   Mon, 25 Dec 2023 07:31:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 09 Jan 2024 08:32:21 +0000   Mon, 25 Dec 2023 07:31:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 09 Jan 2024 08:32:21 +0000   Mon, 25 Dec 2023 07:31:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 09 Jan 2024 08:32:21 +0000   Mon, 25 Dec 2023 07:31:39 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8020472Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8020472Ki
  pods:               110
System Info:
  Machine ID:                 4be7b89512914632b7eb285a3ba7704a
  System UUID:                4be7b89512914632b7eb285a3ba7704a
  Boot ID:                    f2b24074-6df4-45cb-84be-c3e189be33f4
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://23.0.2
  Kubelet Version:            v1.26.3
  Kube-Proxy Version:         v1.26.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (15 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     nginx-deployment-f5d9744f-lcrvm              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d18h
  default                     nginx-deployment-f5d9744f-slfd6              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d18h
  default                     nginx-deployment-f5d9744f-tt4gw              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d18h
  develop-ghw-sc-vol          ghw-deployment-sc-vol-55f99d7cd5-8lddh       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         59s
  develop-ghw                 ghw-deployment-bd8c5c594-f6ph2               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14d
  kube-system                 coredns-787d4945fb-hkk6k                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     58d
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         58d
  kube-system                 kube-apiserver-minikube                      250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 kube-controller-manager-minikube             200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 kube-proxy-hchxp                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kube-system                 metrics-server-6588d95b98-kdjdj              100m (1%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         48d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         58d
  kubernetes-dashboard        dashboard-metrics-scraper-5c6664855-hphql    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         53d
  kubernetes-dashboard        kubernetes-dashboard-55c4cbbc7c-n9cr4        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         53d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             370Mi (4%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[  +0.000638] blk_update_request: I/O error, dev sdc, sector 268173312 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000660] blk_update_request: I/O error, dev sdc, sector 268173312 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000551] Buffer I/O error on dev sdc, logical block 33521664, lost sync page write
[  +0.000407] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000446] Aborting journal on device sdc-8.
[  +0.000338] blk_update_request: I/O error, dev sdc, sector 268173312 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000575] blk_update_request: I/O error, dev sdc, sector 268173312 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000616] Buffer I/O error on dev sdc, logical block 33521664, lost sync page write
[  +0.000459] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000487] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000819] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000670] Buffer I/O error on dev sdc, logical block 0, lost sync page write
[  +0.000456] EXT4-fs (sdc): I/O error while writing superblock
[  +0.000391] EXT4-fs error (device sdc): ext4_put_super:1196: Couldn't clean up the journal
[  +0.000490] EXT4-fs (sdc): Remounting filesystem read-only
[  +1.204389] FS-Cache: Duplicate cookie detected
[  +0.000561] FS-Cache: O-cookie c=0000000027bfc779 [p=000000001a688a44 fl=222 nc=0 na=1]
[  +0.000435] FS-Cache: O-cookie d=00000000ae4dfd43 n=000000000ab706b7
[  +0.000418] FS-Cache: O-key=[10] '34323934393532393938'
[  +0.000440] FS-Cache: N-cookie c=000000003b4f62b7 [p=000000001a688a44 fl=2 nc=0 na=1]
[  +0.000491] FS-Cache: N-cookie d=00000000ae4dfd43 n=00000000b18cb08b
[  +0.000361] FS-Cache: N-key=[10] '34323934393532393938'
[  +0.001360] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.011843] FS-Cache: Duplicate cookie detected
[  +0.000413] FS-Cache: O-cookie c=000000003b4f62b7 [p=000000001a688a44 fl=222 nc=0 na=1]
[  +0.000408] FS-Cache: O-cookie d=00000000ae4dfd43 n=00000000d80b9704
[  +0.000345] FS-Cache: O-key=[10] '34323934393533303030'
[  +0.000336] FS-Cache: N-cookie c=00000000db99642b [p=000000001a688a44 fl=2 nc=0 na=1]
[  +0.000450] FS-Cache: N-cookie d=00000000ae4dfd43 n=0000000010760f72
[  +0.000379] FS-Cache: N-key=[10] '34323934393533303030'
[  +0.005751] WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[  +0.987991] FS-Cache: Duplicate cookie detected
[  +0.000548] FS-Cache: O-cookie c=0000000029bbf61e [p=000000001a688a44 fl=222 nc=0 na=1]
[  +0.000557] FS-Cache: O-cookie d=00000000ae4dfd43 n=00000000a26937b9
[  +0.000641] FS-Cache: O-key=[10] '34323934393533303939'
[  +0.000434] FS-Cache: N-cookie c=000000000c6a4a51 [p=000000001a688a44 fl=2 nc=0 na=1]
[  +0.000541] FS-Cache: N-cookie d=00000000ae4dfd43 n=0000000072b41069
[  +0.000505] FS-Cache: N-key=[10] '34323934393533303939'
[  +0.000973] init: (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.002211] init: (2) ERROR: UtilCreateProcessAndWait:702: /bin/mount failed with 2
[  +0.001097] init: (1) ERROR: UtilCreateProcessAndWait:722: /bin/mount failed with status 0x
[  +0.000003] ff00
[  +0.000930] init: (1) ERROR: ConfigMountFsTab:2484: Processing fstab with mount -a failed.
[  +0.009374] 9pnet_virtio: no channels available for device drvfs
[  +0.000472] WARNING: mount: waiting for virtio device...
[  +0.100473] 9pnet_virtio: no channels available for device drvfs
[  +0.112637] WARNING: /usr/share/zoneinfo/Asia/Jakarta not found. Is the tzdata package installed?
[  +0.288793] init: (8) ERROR: CreateProcessEntryCommon:443: getpwuid(0) failed 2
[  +0.004150] init: (8) ERROR: CreateProcessEntryCommon:446: getpwuid(0) failed 2
[Jan 9 02:48] cgroup: runc (910) created nested cgroup for controller "memory" which has incomplete hierarchy support. Nested cgroups may change behavior in the future.
[  +0.003437] cgroup: "memory" requires setting use_hierarchy to 1 on the root
[Jan 9 05:30] hrtimer: interrupt took 5624457 ns
[Jan 9 07:38] init: (191168) ERROR: InitCreateProcessUtilityVm:1743: write failed 32
[  +0.974509] init: (187822) ERROR: InitCreateProcessUtilityVm:1743: write failed 32
[Jan 9 07:42] init: (196799) ERROR: InitCreateProcessUtilityVm:1743: write failed 32
[  +0.031547] init: (196799) ERROR: InitCreateProcessUtilityVm:1743: write failed 32
[  +5.960552] init: (6960) ERROR: InitCreateProcessUtilityVm:1743: write failed 32
[Jan 9 08:10] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.


==> etcd [0bb5326a48d6] <==
{"level":"info","ts":"2024-01-09T07:24:13.883Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":660163,"took":"813.049µs","hash":1044304698}
{"level":"info","ts":"2024-01-09T07:24:13.883Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1044304698,"revision":660163,"compact-revision":659918}
{"level":"info","ts":"2024-01-09T07:29:13.891Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":660415}
{"level":"info","ts":"2024-01-09T07:29:13.892Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":660415,"took":"836.993µs","hash":3372111707}
{"level":"info","ts":"2024-01-09T07:29:13.892Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3372111707,"revision":660415,"compact-revision":660163}
{"level":"info","ts":"2024-01-09T07:34:13.897Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":660662}
{"level":"info","ts":"2024-01-09T07:34:13.898Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":660662,"took":"734.386µs","hash":2340332288}
{"level":"info","ts":"2024-01-09T07:34:13.899Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2340332288,"revision":660662,"compact-revision":660415}
{"level":"info","ts":"2024-01-09T07:39:13.904Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":660907}
{"level":"info","ts":"2024-01-09T07:39:13.905Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":660907,"took":"935.46µs","hash":3476960918}
{"level":"info","ts":"2024-01-09T07:39:13.905Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3476960918,"revision":660907,"compact-revision":660662}
{"level":"info","ts":"2024-01-09T07:44:13.911Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":661159}
{"level":"info","ts":"2024-01-09T07:44:13.913Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":661159,"took":"776.511µs","hash":4144623366}
{"level":"info","ts":"2024-01-09T07:44:13.913Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4144623366,"revision":661159,"compact-revision":660907}
{"level":"info","ts":"2024-01-09T07:49:13.960Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":661409}
{"level":"info","ts":"2024-01-09T07:49:13.962Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":661409,"took":"1.306841ms","hash":1842275567}
{"level":"info","ts":"2024-01-09T07:49:13.962Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1842275567,"revision":661409,"compact-revision":661159}
{"level":"info","ts":"2024-01-09T07:54:13.973Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":661654}
{"level":"info","ts":"2024-01-09T07:54:13.974Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":661654,"took":"680.659µs","hash":2487137987}
{"level":"info","ts":"2024-01-09T07:54:13.974Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2487137987,"revision":661654,"compact-revision":661409}
{"level":"info","ts":"2024-01-09T07:55:56.367Z","caller":"traceutil/trace.go:171","msg":"trace[387982793] transaction","detail":"{read_only:false; response_revision:661979; number_of_response:1; }","duration":"126.063427ms","start":"2024-01-09T07:55:56.241Z","end":"2024-01-09T07:55:56.367Z","steps":["trace[387982793] 'process raft request'  (duration: 115.752076ms)","trace[387982793] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1091; } (duration: 10.144351ms)"],"step_count":2}
{"level":"info","ts":"2024-01-09T07:59:14.020Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":661897}
{"level":"info","ts":"2024-01-09T07:59:14.022Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":661897,"took":"932.58µs","hash":1131097359}
{"level":"info","ts":"2024-01-09T07:59:14.022Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1131097359,"revision":661897,"compact-revision":661654}
{"level":"info","ts":"2024-01-09T08:03:29.705Z","caller":"traceutil/trace.go:171","msg":"trace[17997875] transaction","detail":"{read_only:false; response_revision:662346; number_of_response:1; }","duration":"104.459949ms","start":"2024-01-09T08:03:29.601Z","end":"2024-01-09T08:03:29.705Z","steps":["trace[17997875] 'process raft request'  (duration: 104.30483ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:04:14.064Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":662139}
{"level":"info","ts":"2024-01-09T08:04:14.065Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":662139,"took":"1.109028ms","hash":3081021064}
{"level":"info","ts":"2024-01-09T08:04:14.065Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3081021064,"revision":662139,"compact-revision":661897}
{"level":"info","ts":"2024-01-09T08:09:14.107Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":662382}
{"level":"info","ts":"2024-01-09T08:09:14.108Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":662382,"took":"794.701µs","hash":4223179425}
{"level":"info","ts":"2024-01-09T08:09:14.108Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4223179425,"revision":662382,"compact-revision":662139}
{"level":"info","ts":"2024-01-09T08:12:16.958Z","caller":"traceutil/trace.go:171","msg":"trace[677043035] linearizableReadLoop","detail":"{readStateIndex:833576; appliedIndex:833575; }","duration":"174.058529ms","start":"2024-01-09T08:12:16.783Z","end":"2024-01-09T08:12:16.957Z","steps":["trace[677043035] 'read index received'  (duration: 173.661461ms)","trace[677043035] 'applied index is now lower than readState.Index'  (duration: 395.611µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-09T08:12:16.959Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"174.577755ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" ","response":"range_response_count:1 size:352"}
{"level":"warn","ts":"2024-01-09T08:12:16.959Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.245165ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2024-01-09T08:12:16.959Z","caller":"traceutil/trace.go:171","msg":"trace[780425046] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:1; response_revision:662775; }","duration":"176.027054ms","start":"2024-01-09T08:12:16.783Z","end":"2024-01-09T08:12:16.959Z","steps":["trace[780425046] 'agreement among raft nodes before linearized reading'  (duration: 174.355444ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:12:16.959Z","caller":"traceutil/trace.go:171","msg":"trace[523175737] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:662775; }","duration":"106.670771ms","start":"2024-01-09T08:12:16.853Z","end":"2024-01-09T08:12:16.959Z","steps":["trace[523175737] 'agreement among raft nodes before linearized reading'  (duration: 105.187031ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:12:25.273Z","caller":"traceutil/trace.go:171","msg":"trace[1039833608] transaction","detail":"{read_only:false; response_revision:662782; number_of_response:1; }","duration":"184.369355ms","start":"2024-01-09T08:12:25.088Z","end":"2024-01-09T08:12:25.273Z","steps":["trace[1039833608] 'process raft request'  (duration: 184.256883ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:12:41.722Z","caller":"traceutil/trace.go:171","msg":"trace[1096172367] transaction","detail":"{read_only:false; response_revision:662796; number_of_response:1; }","duration":"283.911062ms","start":"2024-01-09T08:12:41.438Z","end":"2024-01-09T08:12:41.722Z","steps":["trace[1096172367] 'process raft request'  (duration: 283.771652ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:14:14.148Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":662627}
{"level":"info","ts":"2024-01-09T08:14:14.152Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":662627,"took":"2.451078ms","hash":3572044863}
{"level":"info","ts":"2024-01-09T08:14:14.152Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3572044863,"revision":662627,"compact-revision":662382}
{"level":"info","ts":"2024-01-09T08:19:14.159Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":662871}
{"level":"info","ts":"2024-01-09T08:19:14.161Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":662871,"took":"1.238774ms","hash":2824898264}
{"level":"info","ts":"2024-01-09T08:19:14.161Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2824898264,"revision":662871,"compact-revision":662627}
{"level":"info","ts":"2024-01-09T08:24:14.206Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":663114}
{"level":"info","ts":"2024-01-09T08:24:14.208Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":663114,"took":"1.231929ms","hash":1753642652}
{"level":"info","ts":"2024-01-09T08:24:14.208Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1753642652,"revision":663114,"compact-revision":662871}
{"level":"info","ts":"2024-01-09T08:26:18.109Z","caller":"traceutil/trace.go:171","msg":"trace[42516258] transaction","detail":"{read_only:false; response_revision:663666; number_of_response:1; }","duration":"187.028079ms","start":"2024-01-09T08:26:17.921Z","end":"2024-01-09T08:26:18.108Z","steps":["trace[42516258] 'process raft request'  (duration: 186.693448ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:26:23.603Z","caller":"traceutil/trace.go:171","msg":"trace[1358002844] transaction","detail":"{read_only:false; response_revision:663670; number_of_response:1; }","duration":"130.201379ms","start":"2024-01-09T08:26:23.473Z","end":"2024-01-09T08:26:23.603Z","steps":["trace[1358002844] 'process raft request'  (duration: 130.071765ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:26:28.344Z","caller":"traceutil/trace.go:171","msg":"trace[1928379167] transaction","detail":"{read_only:false; response_revision:663675; number_of_response:1; }","duration":"122.730043ms","start":"2024-01-09T08:26:28.221Z","end":"2024-01-09T08:26:28.344Z","steps":["trace[1928379167] 'process raft request'  (duration: 122.574776ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:26:38.605Z","caller":"traceutil/trace.go:171","msg":"trace[1774017950] transaction","detail":"{read_only:false; response_revision:663683; number_of_response:1; }","duration":"101.760138ms","start":"2024-01-09T08:26:38.503Z","end":"2024-01-09T08:26:38.605Z","steps":["trace[1774017950] 'process raft request'  (duration: 101.627654ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:26:46.791Z","caller":"traceutil/trace.go:171","msg":"trace[1213765961] linearizableReadLoop","detail":"{readStateIndex:834679; appliedIndex:834678; }","duration":"230.602688ms","start":"2024-01-09T08:26:46.560Z","end":"2024-01-09T08:26:46.791Z","steps":["trace[1213765961] 'read index received'  (duration: 230.222756ms)","trace[1213765961] 'applied index is now lower than readState.Index'  (duration: 379.216µs)"],"step_count":2}
{"level":"info","ts":"2024-01-09T08:26:46.791Z","caller":"traceutil/trace.go:171","msg":"trace[1754726160] transaction","detail":"{read_only:false; response_revision:663688; number_of_response:1; }","duration":"371.391534ms","start":"2024-01-09T08:26:46.420Z","end":"2024-01-09T08:26:46.791Z","steps":["trace[1754726160] 'process raft request'  (duration: 370.973414ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-09T08:26:46.793Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"230.817175ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-09T08:26:46.793Z","caller":"traceutil/trace.go:171","msg":"trace[1824775407] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:663688; }","duration":"233.010913ms","start":"2024-01-09T08:26:46.560Z","end":"2024-01-09T08:26:46.793Z","steps":["trace[1824775407] 'agreement among raft nodes before linearized reading'  (duration: 230.791659ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-09T08:26:46.795Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-09T08:26:46.420Z","time spent":"371.553602ms","remote":"127.0.0.1:55032","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:663680 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-01-09T08:26:52.999Z","caller":"traceutil/trace.go:171","msg":"trace[538585975] transaction","detail":"{read_only:false; response_revision:663693; number_of_response:1; }","duration":"134.656408ms","start":"2024-01-09T08:26:52.865Z","end":"2024-01-09T08:26:52.999Z","steps":["trace[538585975] 'process raft request'  (duration: 134.57422ms)"],"step_count":1}
{"level":"info","ts":"2024-01-09T08:29:14.251Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":663559}
{"level":"info","ts":"2024-01-09T08:29:14.253Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":663559,"took":"1.886458ms","hash":1172501187}
{"level":"info","ts":"2024-01-09T08:29:14.253Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1172501187,"revision":663559,"compact-revision":663114}


==> etcd [cb712b03f2fe] <==
{"level":"info","ts":"2024-01-08T13:02:53.395Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1011857070,"revision":642640,"compact-revision":642402}
{"level":"info","ts":"2024-01-08T13:07:53.435Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":642877}
{"level":"info","ts":"2024-01-08T13:07:53.436Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":642877,"took":"1.160864ms","hash":3264921638}
{"level":"info","ts":"2024-01-08T13:07:53.436Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3264921638,"revision":642877,"compact-revision":642640}
{"level":"info","ts":"2024-01-08T13:12:53.452Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":643114}
{"level":"info","ts":"2024-01-08T13:12:53.453Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":643114,"took":"1.537823ms","hash":1366929012}
{"level":"info","ts":"2024-01-08T13:12:53.453Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1366929012,"revision":643114,"compact-revision":642877}
{"level":"warn","ts":"2024-01-08T17:04:16.781Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"115.624285ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026354806053945 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:643348 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026354806053943 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-01-08T17:04:16.782Z","caller":"traceutil/trace.go:171","msg":"trace[1869991199] transaction","detail":"{read_only:false; response_revision:643357; number_of_response:1; }","duration":"666.848941ms","start":"2024-01-08T13:12:58.967Z","end":"2024-01-08T17:04:16.782Z","steps":["trace[1869991199] 'process raft request'  (duration: 384.570992ms)","trace[1869991199] 'compare'  (duration: 101.19545ms)"],"step_count":2}
{"level":"info","ts":"2024-01-08T17:04:16.782Z","caller":"traceutil/trace.go:171","msg":"trace[1197252268] linearizableReadLoop","detail":"{readStateIndex:809415; appliedIndex:809414; }","duration":"264.204492ms","start":"2024-01-08T17:04:16.518Z","end":"2024-01-08T17:04:16.782Z","steps":["trace[1197252268] 'read index received'  (duration: 66.4µs)","trace[1197252268] 'applied index is now lower than readState.Index'  (duration: 264.135121ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-08T17:04:16.782Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.128951ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2024-01-08T17:04:16.782Z","caller":"traceutil/trace.go:171","msg":"trace[877358321] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:643357; }","duration":"101.233761ms","start":"2024-01-08T17:04:16.681Z","end":"2024-01-08T17:04:16.782Z","steps":["trace[877358321] 'agreement among raft nodes before linearized reading'  (duration: 100.960906ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-08T17:04:16.783Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"265.199449ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-08T17:04:16.783Z","caller":"traceutil/trace.go:171","msg":"trace[1349187381] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:643357; }","duration":"265.301221ms","start":"2024-01-08T17:04:16.518Z","end":"2024-01-08T17:04:16.783Z","steps":["trace[1349187381] 'agreement among raft nodes before linearized reading'  (duration: 264.470301ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-08T17:04:16.832Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-08T13:12:58.967Z","time spent":"667.051019ms","remote":"127.0.0.1:38916","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:643348 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128026354806053943 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2024-01-08T17:09:10.609Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":643354}
{"level":"info","ts":"2024-01-08T17:09:10.612Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":643354,"took":"1.506227ms","hash":4275015307}
{"level":"info","ts":"2024-01-08T17:09:10.612Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4275015307,"revision":643354,"compact-revision":643114}
{"level":"info","ts":"2024-01-08T17:12:27.591Z","caller":"traceutil/trace.go:171","msg":"trace[1315690570] transaction","detail":"{read_only:false; response_revision:643747; number_of_response:1; }","duration":"109.27441ms","start":"2024-01-08T17:12:27.481Z","end":"2024-01-08T17:12:27.590Z","steps":["trace[1315690570] 'process raft request'  (duration: 109.04204ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-08T17:12:27.898Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"116.252018ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-08T17:12:27.898Z","caller":"traceutil/trace.go:171","msg":"trace[1547683820] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:643747; }","duration":"117.324529ms","start":"2024-01-08T17:12:27.781Z","end":"2024-01-08T17:12:27.898Z","steps":["trace[1547683820] 'count revisions from in-memory index tree'  (duration: 115.856548ms)"],"step_count":1}
{"level":"info","ts":"2024-01-08T17:14:10.654Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":643591}
{"level":"info","ts":"2024-01-08T17:14:10.655Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":643591,"took":"689.181µs","hash":4123612473}
{"level":"info","ts":"2024-01-08T17:14:10.655Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4123612473,"revision":643591,"compact-revision":643354}
{"level":"info","ts":"2024-01-08T17:15:24.288Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":810081,"local-member-snapshot-index":800080,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-01-08T17:15:24.298Z","caller":"etcdserver/server.go:2405","msg":"saved snapshot","snapshot-index":810081}
{"level":"info","ts":"2024-01-08T17:15:24.299Z","caller":"etcdserver/server.go:2435","msg":"compacted Raft logs","compact-index":805081}
{"level":"info","ts":"2024-01-08T17:15:37.221Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000011-00000000000b990c.snap"}
{"level":"info","ts":"2024-01-08T17:19:10.662Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":643828}
{"level":"info","ts":"2024-01-08T17:19:10.663Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":643828,"took":"649.479µs","hash":4077070237}
{"level":"info","ts":"2024-01-08T17:19:10.663Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4077070237,"revision":643828,"compact-revision":643591}
{"level":"info","ts":"2024-01-08T17:24:10.670Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":644071}
{"level":"info","ts":"2024-01-08T17:24:10.672Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":644071,"took":"622.061µs","hash":2641957362}
{"level":"info","ts":"2024-01-08T17:24:10.672Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2641957362,"revision":644071,"compact-revision":643828}
{"level":"info","ts":"2024-01-08T17:29:10.682Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":644362}
{"level":"info","ts":"2024-01-08T17:29:10.683Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":644362,"took":"885.361µs","hash":1982725225}
{"level":"info","ts":"2024-01-08T17:29:10.683Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1982725225,"revision":644362,"compact-revision":644071}
{"level":"info","ts":"2024-01-08T17:34:10.727Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":644621}
{"level":"info","ts":"2024-01-08T17:34:10.728Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":644621,"took":"759.018µs","hash":604546762}
{"level":"info","ts":"2024-01-08T17:34:10.729Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":604546762,"revision":644621,"compact-revision":644362}
{"level":"info","ts":"2024-01-08T17:39:10.741Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":644884}
{"level":"info","ts":"2024-01-08T17:39:10.743Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":644884,"took":"1.987572ms","hash":2464512483}
{"level":"info","ts":"2024-01-08T17:39:10.743Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2464512483,"revision":644884,"compact-revision":644621}
{"level":"info","ts":"2024-01-08T17:42:33.684Z","caller":"traceutil/trace.go:171","msg":"trace[126094714] transaction","detail":"{read_only:false; response_revision:645336; number_of_response:1; }","duration":"105.223061ms","start":"2024-01-08T17:42:33.578Z","end":"2024-01-08T17:42:33.684Z","steps":["trace[126094714] 'process raft request'  (duration: 105.004929ms)"],"step_count":1}
{"level":"info","ts":"2024-01-08T17:44:10.753Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":645148}
{"level":"info","ts":"2024-01-08T17:44:10.756Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":645148,"took":"1.964457ms","hash":3556664526}
{"level":"info","ts":"2024-01-08T17:44:10.756Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3556664526,"revision":645148,"compact-revision":644884}
{"level":"info","ts":"2024-01-08T17:49:10.799Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":645420}
{"level":"info","ts":"2024-01-08T17:49:10.801Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":645420,"took":"1.627583ms","hash":229671298}
{"level":"info","ts":"2024-01-08T17:49:10.801Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":229671298,"revision":645420,"compact-revision":645148}
{"level":"info","ts":"2024-01-08T17:49:33.689Z","caller":"traceutil/trace.go:171","msg":"trace[156813374] transaction","detail":"{read_only:false; response_revision:645711; number_of_response:1; }","duration":"105.984732ms","start":"2024-01-08T17:49:33.583Z","end":"2024-01-08T17:49:33.689Z","steps":["trace[156813374] 'process raft request'  (duration: 105.800967ms)"],"step_count":1}
{"level":"info","ts":"2024-01-08T17:49:47.495Z","caller":"traceutil/trace.go:171","msg":"trace[280135178] transaction","detail":"{read_only:false; response_revision:645722; number_of_response:1; }","duration":"110.16223ms","start":"2024-01-08T17:49:47.384Z","end":"2024-01-08T17:49:47.495Z","steps":["trace[280135178] 'process raft request'  (duration: 109.571538ms)"],"step_count":1}
{"level":"info","ts":"2024-01-08T17:54:10.850Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":645693}
{"level":"info","ts":"2024-01-08T17:54:10.852Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":645693,"took":"1.686136ms","hash":2753650149}
{"level":"info","ts":"2024-01-08T17:54:10.852Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2753650149,"revision":645693,"compact-revision":645420}
{"level":"info","ts":"2024-01-08T17:54:40.286Z","caller":"traceutil/trace.go:171","msg":"trace[1232617055] transaction","detail":"{read_only:false; response_revision:645956; number_of_response:1; }","duration":"107.861915ms","start":"2024-01-08T17:54:40.178Z","end":"2024-01-08T17:54:40.286Z","steps":["trace[1232617055] 'process raft request'  (duration: 107.711599ms)"],"step_count":1}
{"level":"info","ts":"2024-01-08T17:59:10.865Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":645931}
{"level":"info","ts":"2024-01-08T17:59:10.867Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":645931,"took":"2.027428ms","hash":1563723634}
{"level":"info","ts":"2024-01-08T17:59:10.867Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1563723634,"revision":645931,"compact-revision":645693}
{"level":"info","ts":"2024-01-08T18:00:18.453Z","caller":"traceutil/trace.go:171","msg":"trace[352190478] transaction","detail":"{read_only:false; response_revision:646255; number_of_response:1; }","duration":"107.701628ms","start":"2024-01-08T18:00:18.345Z","end":"2024-01-08T18:00:18.453Z","steps":["trace[352190478] 'process raft request'  (duration: 69.276496ms)","trace[352190478] 'compare'  (duration: 38.290615ms)"],"step_count":2}


==> kernel <==
 08:33:11 up  5:47,  0 users,  load average: 1.43, 1.13, 1.03
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"


==> kube-apiserver [bdd4d6518985] <==
I0109 03:19:36.168220       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0109 03:27:37.672814       1 trace.go:219] Trace[162972135]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:fdc77135-dfb7-4b8b-a6ef-d7f1f488b00a,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:PUT (09-Jan-2024 03:27:31.402) (total time: 6202ms):
Trace[162972135]: ["GuaranteedUpdate etcd3" audit-id:fdc77135-dfb7-4b8b-a6ef-d7f1f488b00a,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 6201ms (03:27:31.403)
Trace[162972135]:  ---"Txn call completed" 6198ms (03:27:37.672)]
Trace[162972135]: [6.202148412s] [6.202148412s] END
I0109 03:27:38.066025       1 trace.go:219] Trace[983203866]: "Update" accept:application/json, */*,audit-id:7e2c289e-5548-4fd7-8b9f-9245ee1f60c0,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Jan-2024 03:27:33.473) (total time: 4098ms):
Trace[983203866]: ["GuaranteedUpdate etcd3" audit-id:7e2c289e-5548-4fd7-8b9f-9245ee1f60c0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 4097ms (03:27:33.474)
Trace[983203866]:  ---"Txn call completed" 4084ms (03:27:37.571)]
Trace[983203866]: [4.098579603s] [4.098579603s] END
{"level":"warn","ts":"2024-01-09T03:27:36.766Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00054b6c0/127.0.0.1:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
I0109 03:27:39.777151       1 trace.go:219] Trace[295749529]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:eceb80fb-da5f-4b1c-86cc-60a4444526a2,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-apiserver-kcdegeihzlqbkh65rirrc5zxju,user-agent:kube-apiserver/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:PUT (09-Jan-2024 03:27:33.869) (total time: 5907ms):
Trace[295749529]: ["GuaranteedUpdate etcd3" audit-id:eceb80fb-da5f-4b1c-86cc-60a4444526a2,key:/leases/kube-system/kube-apiserver-kcdegeihzlqbkh65rirrc5zxju,type:*coordination.Lease,resource:leases.coordination.k8s.io 5901ms (03:27:33.875)
Trace[295749529]:  ---"Txn call completed" 5895ms (03:27:39.776)]
Trace[295749529]: [5.907796353s] [5.907796353s] END
I0109 03:27:39.971596       1 trace.go:219] Trace[2008497848]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3c694694-7345-4ec8-a696-eb52bc567abc,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:GET (09-Jan-2024 03:27:36.169) (total time: 3802ms):
Trace[2008497848]: ---"About to write a response" 3802ms (03:27:39.971)
Trace[2008497848]: [3.802442964s] [3.802442964s] END
I0109 03:27:41.265958       1 trace.go:219] Trace[1365036149]: "Get" accept:application/json, */*,audit-id:2faabd8e-e4ac-4951-b1cd-f77dcf6d4012,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (09-Jan-2024 03:27:40.282) (total time: 983ms):
Trace[1365036149]: ---"About to write a response" 983ms (03:27:41.265)
Trace[1365036149]: [983.438386ms] [983.438386ms] END
I0109 03:27:41.767512       1 trace.go:219] Trace[303618896]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:414332c6-04cc-4e60-8893-729e2977299f,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/kube-system/events/metrics-server-6588d95b98-kdjdj.17a88dfaf22f8ec5,user-agent:kubelet/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:PATCH (09-Jan-2024 03:27:38.370) (total time: 3396ms):
Trace[303618896]: ["GuaranteedUpdate etcd3" audit-id:414332c6-04cc-4e60-8893-729e2977299f,key:/events/kube-system/metrics-server-6588d95b98-kdjdj.17a88dfaf22f8ec5,type:*core.Event,resource:events 3396ms (03:27:38.370)
Trace[303618896]:  ---"initial value restored" 1599ms (03:27:39.970)
Trace[303618896]:  ---"Transaction prepared" 592ms (03:27:40.566)
Trace[303618896]:  ---"Txn call completed" 1200ms (03:27:41.766)]
Trace[303618896]: ---"Object stored in database" 1794ms (03:27:41.767)
Trace[303618896]: [3.396913479s] [3.396913479s] END
I0109 03:27:44.066289       1 trace.go:219] Trace[1025882371]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Jan-2024 03:27:39.991) (total time: 4074ms):
Trace[1025882371]: ---"initial value restored" 574ms (03:27:40.566)
Trace[1025882371]: ---"Transaction prepared" 1200ms (03:27:41.766)
Trace[1025882371]: ---"Txn call completed" 499ms (03:27:42.266)
Trace[1025882371]: ---"Transaction prepared" 599ms (03:27:42.866)
Trace[1025882371]: ---"Txn call completed" 1200ms (03:27:44.066)
Trace[1025882371]: [4.074604884s] [4.074604884s] END
I0109 03:27:44.871567       1 trace.go:219] Trace[1769035070]: "Update" accept:application/json, */*,audit-id:30a272fd-d77a-443c-8f97-a57353cb4e84,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Jan-2024 03:27:44.089) (total time: 782ms):
Trace[1769035070]: ---"Conversion done" 86ms (03:27:44.176)
Trace[1769035070]: ["GuaranteedUpdate etcd3" audit-id:30a272fd-d77a-443c-8f97-a57353cb4e84,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 694ms (03:27:44.177)
Trace[1769035070]:  ---"About to Encode" 93ms (03:27:44.271)
Trace[1769035070]:  ---"Txn call completed" 599ms (03:27:44.871)]
Trace[1769035070]: [782.240432ms] [782.240432ms] END
I0109 03:27:44.871908       1 trace.go:219] Trace[373945838]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4ede12f6-d1d6-40d9-8781-c7b1414b519b,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:GET (09-Jan-2024 03:27:44.087) (total time: 783ms):
Trace[373945838]: ---"About to write a response" 783ms (03:27:44.871)
Trace[373945838]: [783.917261ms] [783.917261ms] END
I0109 03:27:46.051166       1 trace.go:219] Trace[1108963855]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Jan-2024 03:27:45.479) (total time: 571ms):
Trace[1108963855]: ---"Transaction prepared" 130ms (03:27:45.613)
Trace[1108963855]: ---"Txn call completed" 437ms (03:27:46.050)
Trace[1108963855]: [571.731393ms] [571.731393ms] END
I0109 03:27:48.168611       1 trace.go:219] Trace[2017054310]: "Update" accept:application/json, */*,audit-id:6c9e20c7-1d66-45c4-987e-26e17107e7ed,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Jan-2024 03:27:46.898) (total time: 1269ms):
Trace[2017054310]: ["GuaranteedUpdate etcd3" audit-id:6c9e20c7-1d66-45c4-987e-26e17107e7ed,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1268ms (03:27:46.899)
Trace[2017054310]:  ---"Txn call completed" 1255ms (03:27:48.167)]
Trace[2017054310]: [1.269840218s] [1.269840218s] END
I0109 03:27:56.201204       1 trace.go:219] Trace[31110441]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Jan-2024 03:27:55.669) (total time: 531ms):
Trace[31110441]: ---"initial value restored" 96ms (03:27:55.766)
Trace[31110441]: ---"Transaction prepared" 121ms (03:27:55.888)
Trace[31110441]: ---"Txn call completed" 312ms (03:27:56.200)
Trace[31110441]: [531.31382ms] [531.31382ms] END
I0109 03:27:57.777519       1 trace.go:219] Trace[365885150]: "Update" accept:application/json, */*,audit-id:5e409f7c-c60a-46d5-affe-8f91ac447523,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Jan-2024 03:27:57.073) (total time: 704ms):
Trace[365885150]: ["GuaranteedUpdate etcd3" audit-id:5e409f7c-c60a-46d5-affe-8f91ac447523,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 703ms (03:27:57.074)
Trace[365885150]:  ---"Txn call completed" 700ms (03:27:57.777)]
Trace[365885150]: [704.261563ms] [704.261563ms] END


==> kube-apiserver [f22685ea40e1] <==
I0108 07:17:11.841005       1 trace.go:219] Trace[1110155875]: "Get" accept:application/json, */*,audit-id:13e6f62d-b32e-4e5d-a1c9-a931ec200c12,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (08-Jan-2024 07:17:11.208) (total time: 632ms):
Trace[1110155875]: ---"About to write a response" 632ms (07:17:11.840)
Trace[1110155875]: [632.668231ms] [632.668231ms] END
I0108 07:17:16.627055       1 trace.go:219] Trace[733431439]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:af7c1b07-536e-4382-8504-b6fabca332fb,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:PUT (08-Jan-2024 07:17:16.050) (total time: 576ms):
Trace[733431439]: ---"limitedReadBody succeeded" len:477 57ms (07:17:16.108)
Trace[733431439]: ["GuaranteedUpdate etcd3" audit-id:af7c1b07-536e-4382-8504-b6fabca332fb,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 518ms (07:17:16.108)
Trace[733431439]:  ---"Txn call completed" 515ms (07:17:16.626)]
Trace[733431439]: [576.387913ms] [576.387913ms] END
I0108 07:17:17.820718       1 trace.go:219] Trace[395494563]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Jan-2024 07:17:17.234) (total time: 585ms):
Trace[395494563]: ---"Transaction prepared" 299ms (07:17:17.542)
Trace[395494563]: ---"Txn call completed" 277ms (07:17:17.820)
Trace[395494563]: [585.977805ms] [585.977805ms] END
I0108 07:17:17.821265       1 trace.go:219] Trace[960144359]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3d7c1000-ea85-413d-96ad-3c44c38d525f,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-apiserver-kcdegeihzlqbkh65rirrc5zxju,user-agent:kube-apiserver/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:PUT (08-Jan-2024 07:17:17.234) (total time: 586ms):
Trace[960144359]: ["GuaranteedUpdate etcd3" audit-id:3d7c1000-ea85-413d-96ad-3c44c38d525f,key:/leases/kube-system/kube-apiserver-kcdegeihzlqbkh65rirrc5zxju,type:*coordination.Lease,resource:leases.coordination.k8s.io 585ms (07:17:17.235)
Trace[960144359]:  ---"Txn call completed" 583ms (07:17:17.820)]
Trace[960144359]: [586.214694ms] [586.214694ms] END
I0108 07:17:21.201359       1 trace.go:219] Trace[482829400]: "Update" accept:application/json, */*,audit-id:4a22b7ea-1511-4fba-a8ad-d5774b6ab9a6,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Jan-2024 07:17:20.344) (total time: 856ms):
Trace[482829400]: ["GuaranteedUpdate etcd3" audit-id:4a22b7ea-1511-4fba-a8ad-d5774b6ab9a6,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 855ms (07:17:20.346)
Trace[482829400]:  ---"Txn call completed" 852ms (07:17:21.200)]
Trace[482829400]: [856.774265ms] [856.774265ms] END
I0108 07:17:55.829084       1 trace.go:219] Trace[120876315]: "Update" accept:application/json, */*,audit-id:7e468e69-b25e-4a46-88b8-5ad4e8f92685,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Jan-2024 07:17:54.860) (total time: 968ms):
Trace[120876315]: ---"About to convert to expected version" 63ms (07:17:54.924)
Trace[120876315]: ["GuaranteedUpdate etcd3" audit-id:7e468e69-b25e-4a46-88b8-5ad4e8f92685,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 904ms (07:17:54.924)
Trace[120876315]:  ---"Txn call completed" 901ms (07:17:55.828)]
Trace[120876315]: [968.565198ms] [968.565198ms] END
I0108 07:17:58.024316       1 trace.go:219] Trace[1688661333]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:3a0b37bb-743f-4196-81dc-8a20f1f4291d,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:PUT (08-Jan-2024 07:17:57.358) (total time: 665ms):
Trace[1688661333]: ["GuaranteedUpdate etcd3" audit-id:3a0b37bb-743f-4196-81dc-8a20f1f4291d,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 665ms (07:17:57.359)
Trace[1688661333]:  ---"About to Encode" 65ms (07:17:57.424)
Trace[1688661333]:  ---"Txn call completed" 599ms (07:17:58.023)]
Trace[1688661333]: [665.476164ms] [665.476164ms] END
I0108 07:18:01.531870       1 trace.go:219] Trace[1867399286]: "Update" accept:application/json, */*,audit-id:7ae376b8-dc1b-46e8-b905-5b4a0afff4e0,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Jan-2024 07:18:00.929) (total time: 602ms):
Trace[1867399286]: ["GuaranteedUpdate etcd3" audit-id:7ae376b8-dc1b-46e8-b905-5b4a0afff4e0,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 600ms (07:18:00.931)
Trace[1867399286]:  ---"Txn call completed" 597ms (07:18:01.531)]
Trace[1867399286]: [602.015847ms] [602.015847ms] END
I0108 07:18:17.752553       1 trace.go:219] Trace[1684588956]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Jan-2024 07:18:17.173) (total time: 578ms):
Trace[1684588956]: ---"Transaction prepared" 120ms (07:18:17.299)
Trace[1684588956]: ---"Txn call completed" 453ms (07:18:17.752)
Trace[1684588956]: [578.985885ms] [578.985885ms] END
I0108 09:13:54.162687       1 trace.go:219] Trace[1527997782]: "Update" accept:application/json, */*,audit-id:e9582a84-b226-48b9-8371-ccbe3195414b,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-Jan-2024 08:57:29.447) (total time: 1268ms):
Trace[1527997782]: ["GuaranteedUpdate etcd3" audit-id:e9582a84-b226-48b9-8371-ccbe3195414b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1267ms (08:57:29.448)
Trace[1527997782]:  ---"Txn call completed" 1265ms (09:13:54.162)]
Trace[1527997782]: [1.268757055s] [1.268757055s] END
E0108 09:25:08.916685       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0108 09:25:08.918790       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0108 10:36:33.177640       1 trace.go:219] Trace[1953819657]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4edb63cd-7f46-4032-bc82-093510a6d6e6,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.26.3 (linux/amd64) kubernetes/9e64410,verb:GET (08-Jan-2024 10:36:31.863) (total time: 1307ms):
Trace[1953819657]: ---"About to write a response" 1307ms (10:36:33.171)
Trace[1953819657]: [1.307959683s] [1.307959683s] END
E0108 12:03:28.789495       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0108 12:03:28.793671       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0108 17:04:16.882663       1 trace.go:219] Trace[1297974569]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (08-Jan-2024 13:12:58.958) (total time: 727ms):
Trace[1297974569]: ---"Txn call completed" 718ms (17:04:16.833)
Trace[1297974569]: [727.707939ms] [727.707939ms] END
E0108 17:04:18.338924       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0108 17:04:18.340705       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0108 17:16:58.018959       1 controller.go:615] quota admission added evaluator for: namespaces
I0108 17:21:22.752231       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0108 17:21:57.666143       1 alloc.go:327] "allocated clusterIPs" service="develop-ghw-sc-vol/ghw-service-sc-vol" clusterIPs=map[IPv4:10.97.6.95]
I0108 17:30:45.072933       1 alloc.go:327] "allocated clusterIPs" service="develop-ghw-sc-vol/ghw-service-sc-volume" clusterIPs=map[IPv4:10.100.196.65]
I0108 17:31:36.284604       1 alloc.go:327] "allocated clusterIPs" service="develop-ghw-sc-vol/ghw-service-sc-vol" clusterIPs=map[IPv4:10.106.103.10]
I0108 17:47:44.190347       1 alloc.go:327] "allocated clusterIPs" service="develop-ghw-sc-vol/ghw-service-sc-vol" clusterIPs=map[IPv4:10.98.117.145]


==> kube-controller-manager [c5ed3e194fa1] <==
I0108 06:47:35.797070       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I0108 06:47:35.797096       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0108 06:47:35.797266       1 shared_informer.go:280] Caches are synced for PV protection
E0108 06:47:35.805826       1 memcache.go:121] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I0108 06:47:35.811527       1 shared_informer.go:280] Caches are synced for node
I0108 06:47:35.813051       1 range_allocator.go:167] Sending events to api server.
I0108 06:47:35.813885       1 range_allocator.go:171] Starting range CIDR allocator
I0108 06:47:35.813903       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I0108 06:47:35.813920       1 shared_informer.go:280] Caches are synced for cidrallocator
I0108 06:47:35.814921       1 shared_informer.go:280] Caches are synced for TTL
I0108 06:47:35.817203       1 shared_informer.go:280] Caches are synced for GC
I0108 06:47:35.890871       1 shared_informer.go:280] Caches are synced for ReplicationController
I0108 06:47:35.891275       1 shared_informer.go:280] Caches are synced for resource quota
I0108 06:47:35.891484       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I0108 06:47:35.891770       1 shared_informer.go:280] Caches are synced for taint
I0108 06:47:35.896148       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0108 06:47:35.896463       1 shared_informer.go:280] Caches are synced for cronjob
I0108 06:47:35.896979       1 taint_manager.go:211] "Sending events to api server"
I0108 06:47:35.897107       1 shared_informer.go:280] Caches are synced for job
I0108 06:47:35.898273       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
I0108 06:47:35.898900       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0108 06:47:35.899191       1 shared_informer.go:280] Caches are synced for crt configmap
W0108 06:47:35.900596       1 node_lifecycle_controller.go:1053] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0108 06:47:35.900756       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I0108 06:47:35.904093       1 shared_informer.go:280] Caches are synced for attach detach
I0108 06:47:35.907437       1 shared_informer.go:280] Caches are synced for PVC protection
I0108 06:47:35.907634       1 shared_informer.go:280] Caches are synced for resource quota
I0108 06:47:35.909915       1 shared_informer.go:280] Caches are synced for disruption
I0108 06:47:35.891280       1 shared_informer.go:280] Caches are synced for daemon sets
I0108 06:47:35.914431       1 shared_informer.go:280] Caches are synced for deployment
I0108 06:47:35.914664       1 shared_informer.go:280] Caches are synced for endpoint_slice
I0108 06:47:35.914704       1 shared_informer.go:280] Caches are synced for persistent volume
I0108 06:47:35.914722       1 shared_informer.go:280] Caches are synced for HPA
I0108 06:47:35.914724       1 shared_informer.go:280] Caches are synced for stateful set
I0108 06:47:35.914741       1 shared_informer.go:280] Caches are synced for ReplicaSet
I0108 06:47:36.008161       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I0108 06:47:36.101610       1 shared_informer.go:280] Caches are synced for ephemeral
I0108 06:47:36.102061       1 shared_informer.go:280] Caches are synced for endpoint
I0108 06:47:36.294553       1 shared_informer.go:280] Caches are synced for garbage collector
I0108 06:47:36.302542       1 shared_informer.go:280] Caches are synced for garbage collector
I0108 06:47:36.302577       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
W0108 09:25:08.917790       1 garbagecollector.go:754] failed to discover preferred resources: the server has asked for the client to provide credentials
E0108 09:25:08.919461       1 resource_quota_controller.go:417] failed to discover resources: the server has asked for the client to provide credentials
E0108 12:03:28.804079       1 resource_quota_controller.go:417] failed to discover resources: the server has asked for the client to provide credentials
W0108 12:03:29.008174       1 garbagecollector.go:754] failed to discover preferred resources: the server has asked for the client to provide credentials
E0108 17:04:18.340323       1 resource_quota_controller.go:417] failed to discover resources: the server has asked for the client to provide credentials
W0108 17:04:18.349013       1 garbagecollector.go:754] failed to discover preferred resources: the server has asked for the client to provide credentials
I0108 17:21:22.763409       1 event.go:294] "Event occurred" object="default/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-65d5878f8 to 1"
I0108 17:21:22.794622       1 event.go:294] "Event occurred" object="default/ghw-deployment-sc-vol-65d5878f8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-65d5878f8-jpw65"
I0108 17:21:44.447257       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-65d5878f8 to 1"
I0108 17:21:44.463063       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-65d5878f8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-65d5878f8-nx4jx"
I0108 17:28:37.307062       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-65d5878f8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-65d5878f8-4drfw"
I0108 17:34:43.035220       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-65d5878f8 to 1"
I0108 17:34:43.048114       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-65d5878f8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-65d5878f8-cbgrx"
I0108 17:40:46.803538       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-66697d7c5d to 1"
I0108 17:40:46.816836       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-66697d7c5d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-66697d7c5d-vkpfr"
I0108 17:46:13.398198       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-7c8897c8b to 1"
I0108 17:46:13.419447       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-7c8897c8b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-7c8897c8b-n8p4k"
I0108 17:55:52.840070       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-7c8897c8b to 1"
I0108 17:55:52.858418       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-7c8897c8b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-7c8897c8b-v2cs7"


==> kube-controller-manager [ef434ffd62fb] <==
I0109 04:04:36.267938       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-77fd89965f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-77fd89965f-lg6xs"
I0109 04:07:41.937296       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-6f7889ffc9 to 1"
I0109 04:07:41.949713       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-6f7889ffc9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-6f7889ffc9-lchrc"
I0109 04:07:44.126370       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-77fd89965f to 0 from 1"
I0109 04:07:44.140794       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-77fd89965f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-77fd89965f-lg6xs"
I0109 04:07:44.218602       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-service-sc-vol" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint develop-ghw-sc-vol/ghw-service-sc-vol: Operation cannot be fulfilled on endpoints \"ghw-service-sc-vol\": the object has been modified; please apply your changes to the latest version and try again"
I0109 06:19:35.559954       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-6f7889ffc9 to 1"
I0109 06:19:35.584088       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-6f7889ffc9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-6f7889ffc9-ctprc"
I0109 06:20:48.898609       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-77fd89965f to 1"
I0109 06:20:48.909315       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-77fd89965f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-77fd89965f-ssk5b"
I0109 06:25:26.577609       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-77fd89965f to 0 from 1"
I0109 06:25:26.585129       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-77fd89965f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-77fd89965f-ssk5b"
I0109 06:25:26.590690       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-778566d7c4 to 1 from 0"
I0109 06:25:26.613136       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-778566d7c4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-778566d7c4-s5xkg"
I0109 06:25:49.720838       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-778566d7c4 to 0 from 1"
I0109 06:25:49.730462       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-778566d7c4" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-778566d7c4-s5xkg"
I0109 06:25:49.739520       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-84b988c6b8 to 1 from 0"
I0109 06:25:49.796582       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-84b988c6b8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-84b988c6b8-8978d"
I0109 06:25:51.756619       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-6f7889ffc9 to 0 from 1"
I0109 06:25:51.763288       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-6f7889ffc9" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-6f7889ffc9-ctprc"
I0109 06:35:09.083527       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-75dc49fcf8 to 1"
I0109 06:35:09.100640       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-75dc49fcf8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-75dc49fcf8-wmdsp"
I0109 06:36:23.997288       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-75dc49fcf8 to 0 from 1"
I0109 06:36:24.006836       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-75dc49fcf8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-75dc49fcf8-wmdsp"
I0109 06:36:24.023629       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-88cc68988 to 1 from 0"
I0109 06:36:24.096835       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-88cc68988" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-88cc68988-p74fn"
I0109 06:36:38.774301       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-88cc68988 to 0 from 1"
I0109 06:36:38.787871       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-88cc68988" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-88cc68988-p74fn"
I0109 06:36:38.793162       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-75dc49fcf8 to 1 from 0"
I0109 06:36:38.801038       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-75dc49fcf8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-75dc49fcf8-htqpb"
I0109 06:37:11.897328       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-75dc49fcf8 to 0 from 1"
I0109 06:37:11.904425       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-75dc49fcf8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-75dc49fcf8-htqpb"
I0109 06:37:11.912644       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-959c85f74 to 1 from 0"
I0109 06:37:11.922175       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-959c85f74" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-959c85f74-9srxh"
W0109 06:40:23.700169       1 endpointslice_controller.go:302] Error syncing endpoint slices for service "develop-ghw-sc-vol/ghw-service-sc-vol", retrying. Error: EndpointSlice informer cache is out of date
W0109 06:40:25.518271       1 endpointslice_controller.go:302] Error syncing endpoint slices for service "develop-ghw-sc-vol/ghw-service-sc-vol", retrying. Error: EndpointSlice informer cache is out of date
I0109 06:40:29.698680       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-959c85f74 to 1"
I0109 06:40:29.716381       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-959c85f74" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-959c85f74-w9lqz"
I0109 06:41:05.008356       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-dc687568d to 1"
I0109 06:41:05.016759       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-dc687568d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-dc687568d-hs5m9"
I0109 06:41:24.354330       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-959c85f74 to 0 from 1"
I0109 06:41:24.364762       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-959c85f74" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-959c85f74-w9lqz"
I0109 06:41:24.374186       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-6d95bf59bd to 1 from 0"
I0109 06:41:24.402801       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-6d95bf59bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-6d95bf59bd-hhh6x"
I0109 08:20:34.516990       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-dc687568d to 0 from 1"
I0109 08:20:34.530632       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-dc687568d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-dc687568d-hs5m9"
I0109 08:20:34.540399       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-59d645b5fb to 1 from 0"
I0109 08:20:34.610865       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-59d645b5fb-9qz4c"
I0109 08:21:29.717330       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-59d645b5fb to 1"
I0109 08:21:29.801329       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-59d645b5fb-dmjqt"
I0109 08:22:14.967836       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-78bb686676 to 1"
I0109 08:22:14.986343       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-78bb686676" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-78bb686676-hmwst"
I0109 08:22:42.667550       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-59d645b5fb to 1"
I0109 08:22:42.677048       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-59d645b5fb-pfx8h"
I0109 08:31:50.201043       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-55f99d7cd5 to 1"
I0109 08:31:50.212826       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-55f99d7cd5-6wbzf"
I0109 08:32:07.383275       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ghw-deployment-sc-vol-59d645b5fb to 0 from 1"
I0109 08:32:07.394629       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ghw-deployment-sc-vol-59d645b5fb-pfx8h"
I0109 08:32:11.349792       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ghw-deployment-sc-vol-55f99d7cd5 to 1"
I0109 08:32:11.359073       1 event.go:294] "Event occurred" object="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ghw-deployment-sc-vol-55f99d7cd5-8lddh"


==> kube-proxy [36e63978f59b] <==
I0108 06:47:28.403120       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0108 06:47:28.403273       1 server_others.go:109] "Detected node IP" address="192.168.49.2"
I0108 06:47:28.403385       1 server_others.go:535] "Using iptables proxy"
I0108 06:47:31.493377       1 server_others.go:176] "Using iptables Proxier"
I0108 06:47:31.493470       1 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0108 06:47:31.493494       1 server_others.go:184] "Creating dualStackProxier for iptables"
I0108 06:47:31.493852       1 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0108 06:47:31.505509       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0108 06:47:31.608153       1 server.go:655] "Version info" version="v1.26.3"
I0108 06:47:31.608640       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0108 06:47:31.716902       1 config.go:317] "Starting service config controller"
I0108 06:47:31.717053       1 config.go:226] "Starting endpoint slice config controller"
I0108 06:47:31.717138       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0108 06:47:31.721129       1 config.go:444] "Starting node config controller"
I0108 06:47:31.721148       1 shared_informer.go:273] Waiting for caches to sync for node config
I0108 06:47:31.721681       1 shared_informer.go:273] Waiting for caches to sync for service config
I0108 06:47:31.896489       1 shared_informer.go:280] Caches are synced for service config
I0108 06:47:31.896607       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0108 06:47:31.896861       1 shared_informer.go:280] Caches are synced for node config


==> kube-proxy [fe680e2c526e] <==
I0109 02:49:25.024686       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0109 02:49:25.024818       1 server_others.go:109] "Detected node IP" address="192.168.49.2"
I0109 02:49:25.024871       1 server_others.go:535] "Using iptables proxy"
I0109 02:49:25.214441       1 server_others.go:176] "Using iptables Proxier"
I0109 02:49:25.214502       1 server_others.go:183] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0109 02:49:25.214513       1 server_others.go:184] "Creating dualStackProxier for iptables"
I0109 02:49:25.214533       1 server_others.go:465] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0109 02:49:25.216370       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0109 02:49:25.217835       1 server.go:655] "Version info" version="v1.26.3"
I0109 02:49:25.217868       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0109 02:49:25.221537       1 config.go:226] "Starting endpoint slice config controller"
I0109 02:49:25.221779       1 config.go:317] "Starting service config controller"
I0109 02:49:25.221906       1 config.go:444] "Starting node config controller"
I0109 02:49:25.222801       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0109 02:49:25.222837       1 shared_informer.go:273] Waiting for caches to sync for service config
I0109 02:49:25.222885       1 shared_informer.go:273] Waiting for caches to sync for node config
I0109 02:49:25.323523       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0109 02:49:25.323593       1 shared_informer.go:280] Caches are synced for service config
I0109 02:49:25.323666       1 shared_informer.go:280] Caches are synced for node config


==> kube-scheduler [1863ee8ebe0d] <==
I0109 02:49:09.815318       1 serving.go:348] Generated self-signed cert in-memory
I0109 02:49:13.806658       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.3"
I0109 02:49:13.806697       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0109 02:49:13.822986       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0109 02:49:13.823184       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0109 02:49:13.823670       1 shared_informer.go:273] Waiting for caches to sync for RequestHeaderAuthRequestController
I0109 02:49:13.824143       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0109 02:49:13.824557       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 02:49:13.824568       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0109 02:49:13.824597       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 02:49:13.824601       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0109 02:49:13.923933       1 shared_informer.go:280] Caches are synced for RequestHeaderAuthRequestController
I0109 02:49:13.925474       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0109 02:49:13.925466       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [4d3f61a51f02] <==
I0108 06:47:02.537088       1 serving.go:348] Generated self-signed cert in-memory
W0108 06:47:14.248155       1 authentication.go:349] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": net/http: TLS handshake timeout
W0108 06:47:14.248233       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W0108 06:47:14.248243       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0108 06:47:14.917013       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.3"
I0108 06:47:14.917105       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0108 06:47:15.001709       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0108 06:47:15.002202       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0108 06:47:15.002805       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0108 06:47:15.002216       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0108 06:47:15.205572       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
-- Logs begin at Tue 2024-01-09 02:48:40 UTC, end at Tue 2024-01-09 08:33:11 UTC. --
Jan 09 08:30:43 minikube kubelet[1905]: I0109 08:30:43.234596    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:30:43 minikube kubelet[1905]: E0109 08:30:43.235103    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:30:54 minikube kubelet[1905]: I0109 08:30:54.233962    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:30:54 minikube kubelet[1905]: E0109 08:30:54.234420    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:31:05 minikube kubelet[1905]: I0109 08:31:05.232969    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:31:05 minikube kubelet[1905]: E0109 08:31:05.233197    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:31:16 minikube kubelet[1905]: I0109 08:31:16.233710    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:31:16 minikube kubelet[1905]: E0109 08:31:16.234155    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:31:28 minikube kubelet[1905]: I0109 08:31:28.233526    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:31:28 minikube kubelet[1905]: E0109 08:31:28.233905    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:31:39 minikube kubelet[1905]: I0109 08:31:39.233972    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:31:39 minikube kubelet[1905]: E0109 08:31:39.234170    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:31:50 minikube kubelet[1905]: I0109 08:31:50.227563    1905 topology_manager.go:210] "Topology Admit Handler"
Jan 09 08:31:50 minikube kubelet[1905]: E0109 08:31:50.227784    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="31cc4b2e-78c2-4296-ba79-e490f1bd86ce" containerName="ghw-container-vol"
Jan 09 08:31:50 minikube kubelet[1905]: E0109 08:31:50.227816    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="79ec67fa-f6fd-4abd-8900-095721eb957f" containerName="ghw-container-vol"
Jan 09 08:31:50 minikube kubelet[1905]: I0109 08:31:50.227936    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="396f8c2e-86f2-49f6-bf15-b164c032c436" containerName="ghw-container-vol"
Jan 09 08:31:50 minikube kubelet[1905]: I0109 08:31:50.227958    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="79ec67fa-f6fd-4abd-8900-095721eb957f" containerName="ghw-container-vol"
Jan 09 08:31:50 minikube kubelet[1905]: I0109 08:31:50.294849    1905 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"secret-volume\" (UniqueName: \"kubernetes.io/secret/929c665c-0599-40b5-9886-d27bbe5395cc-secret-volume\") pod \"ghw-deployment-sc-vol-55f99d7cd5-6wbzf\" (UID: \"929c665c-0599-40b5-9886-d27bbe5395cc\") " pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5-6wbzf"
Jan 09 08:31:50 minikube kubelet[1905]: I0109 08:31:50.395780    1905 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8x9z6\" (UniqueName: \"kubernetes.io/projected/929c665c-0599-40b5-9886-d27bbe5395cc-kube-api-access-8x9z6\") pod \"ghw-deployment-sc-vol-55f99d7cd5-6wbzf\" (UID: \"929c665c-0599-40b5-9886-d27bbe5395cc\") " pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5-6wbzf"
Jan 09 08:31:51 minikube kubelet[1905]: I0109 08:31:51.051854    1905 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6c32500b97015e4f752d1a1dacdcef8e17f25b8106acaa3752b37e62b652f27d"
Jan 09 08:31:51 minikube kubelet[1905]: I0109 08:31:51.234225    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:31:51 minikube kubelet[1905]: E0109 08:31:51.234463    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:32:05 minikube kubelet[1905]: I0109 08:32:05.232859    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:32:05 minikube kubelet[1905]: E0109 08:32:05.233125    1905 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ghw-container-vol\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ghw-container-vol pod=ghw-deployment-sc-vol-59d645b5fb-pfx8h_develop-ghw-sc-vol(7d6b63ba-da80-4a8d-99f6-5ab46584a6d2)\"" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-59d645b5fb-pfx8h" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2
Jan 09 08:32:07 minikube kubelet[1905]: I0109 08:32:07.396091    1905 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5-6wbzf" podStartSLOduration=-9.22337201945945e+09 pod.CreationTimestamp="2024-01-09 08:31:50 +0000 UTC" firstStartedPulling="2024-01-09 08:31:51.266959768 +0000 UTC m=+20566.438894250" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-09 08:32:07.365575411 +0000 UTC m=+20582.538000154" watchObservedRunningTime="2024-01-09 08:32:07.395325408 +0000 UTC m=+20582.567750145"
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.105387    1905 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pf7bq\" (UniqueName: \"kubernetes.io/projected/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2-kube-api-access-pf7bq\") pod \"7d6b63ba-da80-4a8d-99f6-5ab46584a6d2\" (UID: \"7d6b63ba-da80-4a8d-99f6-5ab46584a6d2\") "
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.105528    1905 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"secret-volume\" (UniqueName: \"kubernetes.io/secret/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2-secret-volume\") pod \"7d6b63ba-da80-4a8d-99f6-5ab46584a6d2\" (UID: \"7d6b63ba-da80-4a8d-99f6-5ab46584a6d2\") "
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.110567    1905 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2-secret-volume" (OuterVolumeSpecName: "secret-volume") pod "7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" (UID: "7d6b63ba-da80-4a8d-99f6-5ab46584a6d2"). InnerVolumeSpecName "secret-volume". PluginName "kubernetes.io/secret", VolumeGidValue ""
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.111359    1905 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2-kube-api-access-pf7bq" (OuterVolumeSpecName: "kube-api-access-pf7bq") pod "7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" (UID: "7d6b63ba-da80-4a8d-99f6-5ab46584a6d2"). InnerVolumeSpecName "kube-api-access-pf7bq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.206160    1905 reconciler_common.go:295] "Volume detached for volume \"secret-volume\" (UniqueName: \"kubernetes.io/secret/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2-secret-volume\") on node \"minikube\" DevicePath \"\""
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.206340    1905 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-pf7bq\" (UniqueName: \"kubernetes.io/projected/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2-kube-api-access-pf7bq\") on node \"minikube\" DevicePath \"\""
Jan 09 08:32:08 minikube kubelet[1905]: I0109 08:32:08.371370    1905 scope.go:115] "RemoveContainer" containerID="e72b52a98772c71b9944ebcf49293189bd6a7b60035c11b715ca6fa192725dbe"
Jan 09 08:32:09 minikube kubelet[1905]: I0109 08:32:09.242973    1905 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=7d6b63ba-da80-4a8d-99f6-5ab46584a6d2 path="/var/lib/kubelet/pods/7d6b63ba-da80-4a8d-99f6-5ab46584a6d2/volumes"
Jan 09 08:32:09 minikube kubelet[1905]: I0109 08:32:09.919357    1905 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"secret-volume\" (UniqueName: \"kubernetes.io/secret/929c665c-0599-40b5-9886-d27bbe5395cc-secret-volume\") pod \"929c665c-0599-40b5-9886-d27bbe5395cc\" (UID: \"929c665c-0599-40b5-9886-d27bbe5395cc\") "
Jan 09 08:32:09 minikube kubelet[1905]: I0109 08:32:09.919432    1905 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8x9z6\" (UniqueName: \"kubernetes.io/projected/929c665c-0599-40b5-9886-d27bbe5395cc-kube-api-access-8x9z6\") pod \"929c665c-0599-40b5-9886-d27bbe5395cc\" (UID: \"929c665c-0599-40b5-9886-d27bbe5395cc\") "
Jan 09 08:32:09 minikube kubelet[1905]: I0109 08:32:09.921785    1905 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/929c665c-0599-40b5-9886-d27bbe5395cc-secret-volume" (OuterVolumeSpecName: "secret-volume") pod "929c665c-0599-40b5-9886-d27bbe5395cc" (UID: "929c665c-0599-40b5-9886-d27bbe5395cc"). InnerVolumeSpecName "secret-volume". PluginName "kubernetes.io/secret", VolumeGidValue ""
Jan 09 08:32:09 minikube kubelet[1905]: I0109 08:32:09.921951    1905 operation_generator.go:900] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/929c665c-0599-40b5-9886-d27bbe5395cc-kube-api-access-8x9z6" (OuterVolumeSpecName: "kube-api-access-8x9z6") pod "929c665c-0599-40b5-9886-d27bbe5395cc" (UID: "929c665c-0599-40b5-9886-d27bbe5395cc"). InnerVolumeSpecName "kube-api-access-8x9z6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 09 08:32:10 minikube kubelet[1905]: I0109 08:32:10.020460    1905 reconciler_common.go:295] "Volume detached for volume \"secret-volume\" (UniqueName: \"kubernetes.io/secret/929c665c-0599-40b5-9886-d27bbe5395cc-secret-volume\") on node \"minikube\" DevicePath \"\""
Jan 09 08:32:10 minikube kubelet[1905]: I0109 08:32:10.020524    1905 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-8x9z6\" (UniqueName: \"kubernetes.io/projected/929c665c-0599-40b5-9886-d27bbe5395cc-kube-api-access-8x9z6\") on node \"minikube\" DevicePath \"\""
Jan 09 08:32:10 minikube kubelet[1905]: I0109 08:32:10.411649    1905 scope.go:115] "RemoveContainer" containerID="f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26"
Jan 09 08:32:10 minikube kubelet[1905]: I0109 08:32:10.439844    1905 scope.go:115] "RemoveContainer" containerID="f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26"
Jan 09 08:32:10 minikube kubelet[1905]: E0109 08:32:10.442932    1905 remote_runtime.go:415] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error: No such container: f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26" containerID="f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26"
Jan 09 08:32:10 minikube kubelet[1905]: I0109 08:32:10.443283    1905 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={Type:docker ID:f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26} err="failed to get container status \"f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26\": rpc error: code = Unknown desc = Error: No such container: f03e6a6978153ae35952e443c3fafb1501fd5d2d135417f956bf7ed8a05b7b26"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.250316    1905 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=929c665c-0599-40b5-9886-d27bbe5395cc path="/var/lib/kubelet/pods/929c665c-0599-40b5-9886-d27bbe5395cc/volumes"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.365824    1905 topology_manager.go:210] "Topology Admit Handler"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.365957    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.365975    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.365985    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.365993    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.366001    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.366009    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: E0109 08:32:11.366019    1905 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="929c665c-0599-40b5-9886-d27bbe5395cc" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.366058    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.366071    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.366081    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.366091    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="7d6b63ba-da80-4a8d-99f6-5ab46584a6d2" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.366102    1905 memory_manager.go:346] "RemoveStaleState removing state" podUID="929c665c-0599-40b5-9886-d27bbe5395cc" containerName="ghw-container-vol"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.432179    1905 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9k9l2\" (UniqueName: \"kubernetes.io/projected/0fec9f02-273a-4cbb-9b2a-8817dd347882-kube-api-access-9k9l2\") pod \"ghw-deployment-sc-vol-55f99d7cd5-8lddh\" (UID: \"0fec9f02-273a-4cbb-9b2a-8817dd347882\") " pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5-8lddh"
Jan 09 08:32:11 minikube kubelet[1905]: I0109 08:32:11.432258    1905 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"secret-volume\" (UniqueName: \"kubernetes.io/secret/0fec9f02-273a-4cbb-9b2a-8817dd347882-secret-volume\") pod \"ghw-deployment-sc-vol-55f99d7cd5-8lddh\" (UID: \"0fec9f02-273a-4cbb-9b2a-8817dd347882\") " pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5-8lddh"
Jan 09 08:32:13 minikube kubelet[1905]: I0109 08:32:13.570896    1905 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="develop-ghw-sc-vol/ghw-deployment-sc-vol-55f99d7cd5-8lddh" podStartSLOduration=2.570853139 pod.CreationTimestamp="2024-01-09 08:32:11 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-01-09 08:32:13.570696614 +0000 UTC m=+20588.743121345" watchObservedRunningTime="2024-01-09 08:32:13.570853139 +0000 UTC m=+20588.743277880"


==> kubernetes-dashboard [0c72c68063b2] <==
2024/01/08 06:47:37 Using namespace: kubernetes-dashboard
2024/01/08 06:47:37 Using in-cluster config to connect to apiserver
2024/01/08 06:47:37 Using secret token for csrf signing
2024/01/08 06:47:37 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/01/08 06:47:38 Successful initial request to the apiserver, version: v1.26.3
2024/01/08 06:47:38 Generating JWE encryption key
2024/01/08 06:47:38 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/01/08 06:47:38 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/01/08 06:47:38 Initializing JWE encryption key from synchronized object
2024/01/08 06:47:38 Creating in-cluster Sidecar client
2024/01/08 06:47:38 Serving insecurely on HTTP port: 9090
2024/01/08 06:47:38 Successful request to sidecar
2024/01/08 06:47:37 Starting overwatch


==> kubernetes-dashboard [ce637b62183d] <==
2024/01/09 02:49:25 Using namespace: kubernetes-dashboard
2024/01/09 02:49:25 Using in-cluster config to connect to apiserver
2024/01/09 02:49:25 Using secret token for csrf signing
2024/01/09 02:49:25 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/01/09 02:49:29 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/01/09 02:49:29 Successful initial request to the apiserver, version: v1.26.3
2024/01/09 02:49:29 Generating JWE encryption key
2024/01/09 02:49:29 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/01/09 02:49:29 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/01/09 02:49:29 Initializing JWE encryption key from synchronized object
2024/01/09 02:49:29 Creating in-cluster Sidecar client
2024/01/09 02:49:29 Serving insecurely on HTTP port: 9090
2024/01/09 02:49:29 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2024/01/09 02:49:59 Successful request to sidecar
2024/01/09 02:49:25 Starting overwatch


==> storage-provisioner [829b625e6af5] <==
I0109 02:49:22.098853       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0109 02:49:29.362984       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0109 02:49:29.366331       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0109 02:49:46.805339       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0109 02:49:46.805516       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"bac9efbd-5dc5-47b3-aee2-044cbc9e5b95", APIVersion:"v1", ResourceVersion:"646460", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_5d9549c4-f149-4a3b-8252-35e6058e9b0d became leader
I0109 02:49:46.805554       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_5d9549c4-f149-4a3b-8252-35e6058e9b0d!
I0109 02:49:46.906905       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_5d9549c4-f149-4a3b-8252-35e6058e9b0d!


==> storage-provisioner [e49cd0cc7f95] <==
I0108 06:47:52.349597       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0108 06:47:52.367960       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0108 06:47:52.368769       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0108 06:48:09.809291       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0108 06:48:09.809560       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_083597fa-7b56-48bf-9332-98c63be53e3a!
I0108 06:48:09.810042       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"bac9efbd-5dc5-47b3-aee2-044cbc9e5b95", APIVersion:"v1", ResourceVersion:"627613", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_083597fa-7b56-48bf-9332-98c63be53e3a became leader
I0108 06:48:09.911187       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_083597fa-7b56-48bf-9332-98c63be53e3a!

